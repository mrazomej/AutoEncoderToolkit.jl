<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MMD-VAE (InfoVAE) · AutoEncoderToolkit</title><meta name="title" content="MMD-VAE (InfoVAE) · AutoEncoderToolkit"/><meta property="og:title" content="MMD-VAE (InfoVAE) · AutoEncoderToolkit"/><meta property="twitter:title" content="MMD-VAE (InfoVAE) · AutoEncoderToolkit"/><meta name="description" content="Documentation for AutoEncoderToolkit."/><meta property="og:description" content="Documentation for AutoEncoderToolkit."/><meta property="twitter:description" content="Documentation for AutoEncoderToolkit."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.svg" alt="AutoEncoderToolkit logo"/><img class="docs-dark-only" src="../assets/logo-dark.svg" alt="AutoEncoderToolkit logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AutoEncoderToolkit</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><a class="tocitem" href="../encoders/">Encoders &amp; Decoders</a></li><li><a class="tocitem" href="../layers/">Custom Layers</a></li><li><a class="tocitem" href="../ae/">Deterministic Autoencoders</a></li><li><a class="tocitem" href="../vae/">VAE / β-VAE</a></li><li class="is-active"><a class="tocitem" href>MMD-VAE (InfoVAE)</a><ul class="internal"><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li><li><a class="tocitem" href="#MMDVAEstruct"><span><code>MMDVAE</code> struct</span></a></li><li><a class="tocitem" href="#Forward-pass"><span>Forward pass</span></a></li><li><a class="tocitem" href="#Loss-function"><span>Loss function</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Other-Functions"><span>Other Functions</span></a></li></ul></li><li><a class="tocitem" href="../infomaxvae/">InfoMax-VAE</a></li><li><a class="tocitem" href="../hvae/">HVAE</a></li><li><a class="tocitem" href="../rhvae/">RHVAE</a></li><li><a class="tocitem" href="../diffgeo/">Differential Geometry</a></li><li><a class="tocitem" href="../utils/">Utilities</a></li><li><a class="tocitem" href="../guidelines/">Community Guidelines</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>MMD-VAE (InfoVAE)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>MMD-VAE (InfoVAE)</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="MMDVAEsmodule"><a class="docs-heading-anchor" href="#MMDVAEsmodule">MMD-VAE (InfoVAE)</a><a id="MMDVAEsmodule-1"></a><a class="docs-heading-anchor-permalink" href="#MMDVAEsmodule" title="Permalink"></a></h1><p>The Maximum-Mean Discrepancy Variational Autoencoder (MMD-VAE) is a variant of the Variational Autoencoder (VAE) that adds an extra term to the evidence lower bound (ELBO) that aims to maximize the mutual information between the latent space representation and the input data. In particular, the MMD-VAE uses the Maximum-Mean Discrepancy (MMD) as a measure of the &quot;distance&quot; between the latent space distribution and the input data distribution.</p><p>For the implementation of the MMD-VAE in <code>AutoEncoderToolkit.jl</code>, the <a href="#MMDVAEstruct"><code>MMDVAE</code></a> struct inherits directly from the <a href="../vae/#VAEstruct"><code>VAE</code></a> struct and adds the necessary functions to compute the extra terms in the loss function. An <code>MMDVAE</code> object is created by simply passing a <code>VAE</code> object to the constructor. This way, we can use <code>Julia</code>s multiple dispatch to extend the functionality of the <code>VAE</code> object without having to redefine the entire structure.</p><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><blockquote><p>Maximum-Mean Discrepancy Variational Autoencoders Zhao, S., Song, J. &amp; Ermon, S. InfoVAE: Information Maximizing Variational Autoencoders. Preprint at http://arxiv.org/abs/1706.02262 (2018).</p></blockquote><h2 id="MMDVAEstruct"><a class="docs-heading-anchor" href="#MMDVAEstruct"><code>MMDVAE</code> struct</a><a id="MMDVAEstruct-1"></a><a class="docs-heading-anchor-permalink" href="#MMDVAEstruct" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.MMDVAEs.MMDVAE" href="#AutoEncoderToolkit.MMDVAEs.MMDVAE"><code>AutoEncoderToolkit.MMDVAEs.MMDVAE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">`MMDVAE{
    V&lt;:VAE{&lt;:AbstractVariationalEncoder,&lt;:AbstractVariationalDecoder}
    } &lt;: AbstractVariationalAutoEncoder`</code></pre><p>A struct representing a Maximum-Mean Discrepancy Variational Autoencoder (MMD-VAE).</p><p><strong>Fields</strong></p><ul><li><code>vae::V</code>: A Variational Autoencoder (VAE) that forms the basis of the MMD-VAE. The VAE should be composed of an <code>AbstractVariationalEncoder</code> and an <code>AbstractVariationalDecoder</code>.</li></ul><p><strong>Description</strong></p><p>The <code>MMDVAE</code> struct is a subtype of <code>AbstractVariationalAutoEncoder</code> and represents a specific type of VAE known as an MMD-VAE. The MMD-VAE modifies the standard VAE by replacing the KL-divergence term in the loss function with a Maximum-Mean Discrepancy (MMD) term, which measures the distance between the aggregated posterior of the latent codes and the prior. This can help to alleviate the issue of posterior collapse, where the aggregated posterior fails to cover significant parts of the prior, commonly seen in VAEs.</p><p><strong>Citation</strong></p><blockquote><p>Maximum-Mean Discrepancy Variational Autoencoders. Zhao, S., Song, J. &amp; Ermon, S. InfoVAE: Information Maximizing Variational Autoencoders. Preprint at http://arxiv.org/abs/1706.02262 (2018).</p></blockquote></div></section></article><h2 id="Forward-pass"><a class="docs-heading-anchor" href="#Forward-pass">Forward pass</a><a id="Forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-pass" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.MMDVAEs.MMDVAE-Tuple{AbstractArray}" href="#AutoEncoderToolkit.MMDVAEs.MMDVAE-Tuple{AbstractArray}"><code>AutoEncoderToolkit.MMDVAEs.MMDVAE</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(mmdvae::MMDVAE)(x::AbstractArray; latent::Bool=false)</code></pre><p>Defines the forward pass for the Maximum-Mean Discrepancy Variational Autoencoder (MMD-VAE).</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: Input data.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>latent::Bool</code>: Whether to return the latent variables along with the decoder output. If <code>true</code>, the function returns a tuple containing the encoder outputs, the latent sample, and the decoder outputs. If <code>false</code>, the function only returns the decoder outputs. Defaults to <code>false</code>.  </li></ul><p><strong>Returns</strong></p><ul><li>If <code>latent</code> is <code>true</code>, returns a <code>NamedTuple</code> containing:<ul><li><code>encoder</code>: The outputs of the encoder.</li><li><code>z</code>: The latent sample.</li><li><code>decoder</code>: The outputs of the decoder.</li></ul></li><li>If <code>latent</code> is <code>false</code>, returns the outputs of the decoder.</li></ul></div></section></article><h2 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.MMDVAEs.loss" href="#AutoEncoderToolkit.MMDVAEs.loss"><code>AutoEncoderToolkit.MMDVAEs.loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">loss(mmdvae::MMDVAE, x::AbstractArray; σ::Number=1.0f0, λ::Number=1.0f0, α::Number=0.0f0, n_latent_samples::Int=50, kernel::Function=gaussian_kernel, kernel_kwargs::NamedTuple=NamedTuple(), reconstruction_loglikelihood::Function=decoder_loglikelihood, kl_divergence::Function=encoder_kl)</code></pre><p>Loss function for the Maximum-Mean Discrepancy variational autoencoder (MMD-VAE). The loss function is defined as:</p><p>loss = -⟨log p(x|z)⟩ + (1 - α) * Dₖₗ(qᵩ(z | x) || p(z)) + (λ + α - 1) * MMD-D(qᵩ(z) || p(z)),</p><p><strong>Arguments</strong></p><ul><li><code>mmdvae::MMDVAE</code>: Struct containing the elements of the MMD-VAE.</li><li><code>x::AbstractArray</code>: Input data.</li></ul><p><strong>Optional Arguments</strong></p><ul><li><code>λ::Number=1.0f0</code>: Hyperparameter that emphasizes the importance of the KL divergence between qᵩ(z) and π(z) during training.</li><li><code>α::Number=0.0f0</code>: Hyperparameter that emphasizes the importance of the Mutual Information term during optimization.</li><li><code>n_latent_samples::Int=50</code>: Number of samples to take from the latent space prior π(z) when computing the MMD divergence.</li><li><code>kernel::Function=gaussian_kernel</code>: Kernel used to compute the divergence. Default is the Gaussian Kernel.</li><li><code>kernel_kwargs::NamedTuple=NamedTuple()</code>: Additional keyword arguments to be passed to the kernel function.</li><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: Function that computes the log likelihood of the reconstructed input.</li><li><code>kl_divergence::Function=encoder_kl</code>: Function that computes the Kullback-Leibler divergence between the encoder distribution and the prior.</li></ul><p><strong>Returns</strong></p><ul><li>Single value defining the loss function for entry <code>x</code> when compared with reconstructed output <code>x̂</code>.</li></ul><p><strong>Description</strong></p><p>This function calculates the loss for the MMD-VAE. It computes the log likelihood of the reconstructed input, the MMD divergence between the encoder distribution and the prior, and the Kullback-Leibler divergence between the approximate decoder and the prior. These quantities are combined according to the formula above to compute the loss.</p></div></section><section><div><pre><code class="language-julia hljs">loss(
    mmdvae::MMDVAE, x_in::AbstractArray, x_out::AbstractArray; 
    λ::Number=1.0f0, α::Number=0.0f0, 
    n_latent_samples::Int=50, 
    kernel::Function=gaussian_kernel, 
    kernel_kwargs::NamedTuple=NamedTuple(), 
    reconstruction_loglikelihood::Function=decoder_loglikelihood, 
    kl_divergence::Function=encoder_kl
)</code></pre><p>Loss function for the Maximum-Mean Discrepancy variational autoencoder (MMD-VAE). The loss function is defined as:</p><p>loss = -⟨log p(x|z)⟩ + (1 - α) * Dₖₗ(qᵩ(z | x) || p(z)) + (λ + α - 1) * MMD-D(qᵩ(z) || p(z)),</p><p><strong>Arguments</strong></p><ul><li><code>mmdvae::MMDVAE</code>: Struct containing the elements of the MMD-VAE.</li><li><code>x_in::AbstractArray</code>: Input data.</li><li><code>x_out::AbstractArray</code>: Data against which to compare the reconstructed output.</li></ul><p><strong>Optional Arguments</strong></p><ul><li><code>λ::Number=1.0f0</code>: Hyperparameter that emphasizes the importance of the KL divergence between qᵩ(z) and π(z) during training.</li><li><code>α::Number=0.0f0</code>: Hyperparameter that emphasizes the importance of the Mutual Information term during optimization.</li><li><code>n_latent_samples::Int=50</code>: Number of samples to take from the latent space prior π(z) when computing the MMD divergence.</li><li><code>kernel::Function=gaussian_kernel</code>: Kernel used to compute the divergence. Default is the Gaussian Kernel.</li><li><code>kernel_kwargs::NamedTuple=NamedTuple()</code>: Additional keyword arguments to be passed to the kernel function.</li><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: Function that computes the log likelihood of the reconstructed input.</li><li><code>kl_divergence::Function=encoder_kl</code>: Function that computes the Kullback-Leibler divergence between the encoder distribution and the prior.</li></ul><p><strong>Returns</strong></p><ul><li>Single value defining the loss function for entry <code>x</code> when compared with reconstructed output <code>x̂</code>.</li></ul><p><strong>Description</strong></p><p>This function calculates the loss for the MMD-VAE. It computes the log likelihood of the reconstructed input, the MMD divergence between the encoder distribution and the prior, and the Kullback-Leibler divergence between the approximate decoder and the prior. These quantities are combined according to the formula above to compute the loss.</p></div></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.MMDVAEs.train!" href="#AutoEncoderToolkit.MMDVAEs.train!"><code>AutoEncoderToolkit.MMDVAEs.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train!(mmdvae, x, opt; loss_function, loss_kwargs, verbose, loss_return)</code></pre><p>Customized training function to update parameters of a variational autoencoder given a specified loss function.</p><p><strong>Arguments</strong></p><ul><li><code>mmdvae::MMDVAE</code>: A struct containing the elements of a Maximum-Mean Discrepancy Variational Autoencoder (MMD-VAE).</li><li><code>x::AbstractArray</code>: Data on which to evaluate the loss function. The last dimension is taken as having each of the samples in a batch.</li><li><code>opt::NamedTuple</code>: State of the optimizer for updating parameters. Typically initialized using <code>Flux.Train.setup</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>loss_function::Function=loss</code>: The loss function used for training. It should accept the MMDVAE model, data <code>x</code>, and keyword arguments in that order.</li><li><code>loss_kwargs::NamedTuple=NamedTuple()</code>: Arguments for the loss function. These might include parameters like <code>α</code>, or <code>β</code>, depending on the specific loss function in use.</li><li><code>verbose::Bool=false</code>: If true, the loss value will be printed during training.</li><li><code>loss_return::Bool=false</code>: If true, the loss value will be returned after training.</li></ul><p><strong>Description</strong></p><p>Trains the MMDVAE by:</p><ol><li>Computing the gradient of the loss w.r.t the MMDVAE parameters.</li><li>Updating the MMDVAE parameters using the optimizer.</li></ol></div></section><section><div><pre><code class="language-julia hljs">train!(mmdvae, x_in, x_out, opt; loss_function, loss_kwargs, verbose, loss_return)</code></pre><p>Customized training function to update parameters of a variational autoencoder given a specified loss function.</p><p><strong>Arguments</strong></p><ul><li><code>mmdvae::MMDVAE</code>: A struct containing the elements of a Maximum-Mean Discrepancy Variational Autoencoder (MMD-VAE).</li><li><code>x_in::AbstractArray</code>: Data on which to evaluate the loss function. The last dimension is taken as having each of the samples in a batch.</li><li><code>x_out::AbstractArray</code>: Data against which to compare the reconstructed output.</li><li><code>opt::NamedTuple</code>: State of the optimizer for updating parameters. Typically initialized using <code>Flux.Train.setup</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>loss_function::Function=loss</code>: The loss function used for training. It should accept the MMDVAE model, data <code>x</code>, and keyword arguments in that order.</li><li><code>loss_kwargs::NamedTuple=NamedTuple()</code>: Arguments for the loss function. These might include parameters like <code>α</code>, or <code>β</code>, depending on the specific loss function in use.</li><li><code>verbose::Bool=false</code>: If true, the loss value will be printed during training.</li><li><code>loss_return::Bool=false</code>: If true, the loss value will be returned after training.</li></ul><p><strong>Description</strong></p><p>Trains the MMDVAE by:</p><ol><li>Computing the gradient of the loss w.r.t the MMDVAE parameters.</li><li>Updating the MMDVAE parameters using the optimizer.</li></ol></div></section></article><h2 id="Other-Functions"><a class="docs-heading-anchor" href="#Other-Functions">Other Functions</a><a id="Other-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Other-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.MMDVAEs.gaussian_kernel" href="#AutoEncoderToolkit.MMDVAEs.gaussian_kernel"><code>AutoEncoderToolkit.MMDVAEs.gaussian_kernel</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gaussian_kernel(
    x::AbstractArray, y::AbstractArray; ρ::Float32=1.0f0, dims::Int=2
)</code></pre><p>Function to compute the Gaussian Kernel between two arrays <code>x</code> and <code>y</code>, defined as </p><pre><code class="nohighlight hljs">    k(x, y) = exp(-||x - y ||² / ρ²)</code></pre><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: First input array for the kernel.</li><li><code>y::AbstractArray</code>: Second input array for the kernel.  </li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ρ=1.0f0</code>: Kernel amplitude hyperparameter. Larger ρ gives a smoother kernel.</li><li><code>dims::Int=2</code>: Number of dimensions to compute pairwise distances over.</li></ul><p><strong>Returns</strong></p><ul><li><code>k::AbstractArray</code>: Kernel matrix where each element is computed as </li></ul><p><strong>Theory</strong></p><p>The Gaussian kernel measures the similarity between two points <code>x</code> and <code>y</code>. It is widely used in many machine learning algorithms. This implementation computes the squared Euclidean distance between all pairs of rows in <code>x</code> and <code>y</code>, scales the distance by ρ² and takes the exponential.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.MMDVAEs.mmd_div" href="#AutoEncoderToolkit.MMDVAEs.mmd_div"><code>AutoEncoderToolkit.MMDVAEs.mmd_div</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mmd_div(
    x::AbstractArray, y::AbstractArray; 
    kernel::Function=gaussian_kernel, 
    kernel_kwargs::NamedTuple=NamedTuple()
)</code></pre><p>Compute the Maximum Mean Discrepancy (MMD) divergence between two arrays <code>x</code> and <code>y</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: First input array.</li><li><code>y::AbstractArray</code>: Second input array.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>kernel::Function=gaussian_kernel</code>: Kernel function to use. Default is the Gaussian kernel.</li><li><code>kernel_kwargs::NamedTuple=NamedTuple()</code>: Additional keyword arguments to be passed to the kernel function.</li></ul><p><strong>Returns</strong></p><ul><li><code>mmd::Number</code>: MMD divergence value. </li></ul><p><strong>Theory</strong></p><p>MMD measures the difference between two distributions based on embeddings in a Reproducing Kernel Hilbert Space (RKHS). It is widely used for two-sample tests.</p><p>This function implements MMD as:</p><p>MMD(x, y) = mean(k(x, x)) - 2 * mean(k(x, y)) + mean(k(y, y))</p><p>where k is a positive definite kernel (e.g., Gaussian).</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.MMDVAEs.logP_mmd_ratio" href="#AutoEncoderToolkit.MMDVAEs.logP_mmd_ratio"><code>AutoEncoderToolkit.MMDVAEs.logP_mmd_ratio</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">logP_mmd_ratio(
    mmdvae::MMDVAE, x::AbstractArray; 
    n_latent_samples::Int=100, kernel=gaussian_kernel, 
    kernel_kwargs::NamedTuple=NamedTuple(), 
    reconstruction_loglikelihood::Function=decoder_loglikelihood
)</code></pre><p>Function to compute the absolute ratio between the log likelihood ⟨log p(x|z)⟩ and the MMD divergence MMD-D(qᵩ(z|x)||p(z)).</p><p><strong>Arguments</strong></p><ul><li><code>mmdvae::MMDVAE</code>: Struct containing the elements of the MMD-VAE.</li><li><code>x::AbstractArray</code>: Data to train the MMD-VAE.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>n_latent_samples::Int=100</code>: Number of samples to take from the latent space prior p(z) when computing the MMD divergence.</li><li><code>kernel=gaussian_kernel</code>: Kernel used to compute the divergence.  Default is the Gaussian Kernel.</li><li><code>kernel_kwargs::NamedTuple=NamedTuple()</code>: Tuple containing arguments for the Kernel function.</li><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: Function that computes the log likelihood of the reconstructed input.</li></ul><p><strong>Returns</strong></p><p>abs(⟨log p(x|z)⟩ / MMD-D(qᵩ(z|x)||p(z)))</p><p><strong>Description</strong></p><p>This function calculates:</p><ol><li>The log likelihood ⟨log p(x|z)⟩ of x under the MMD-VAE decoder, averaged over</li></ol><p>all samples. 2. The MMD divergence between the encoder distribution q(z|x) and prior p(z). </p><p>The absolute ratio of these two quantities is returned.</p><p><strong>Note</strong></p><p>This ratio is useful for setting the Lagrangian multiplier λ in training MMD-VAEs.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../vae/">« VAE / β-VAE</a><a class="docs-footer-nextpage" href="../infomaxvae/">InfoMax-VAE »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Tuesday 23 July 2024 19:02">Tuesday 23 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
