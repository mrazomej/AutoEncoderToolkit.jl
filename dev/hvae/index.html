<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>HVAE · AutoEncoderToolkit</title><meta name="title" content="HVAE · AutoEncoderToolkit"/><meta property="og:title" content="HVAE · AutoEncoderToolkit"/><meta property="twitter:title" content="HVAE · AutoEncoderToolkit"/><meta name="description" content="Documentation for AutoEncoderToolkit."/><meta property="og:description" content="Documentation for AutoEncoderToolkit."/><meta property="twitter:description" content="Documentation for AutoEncoderToolkit."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.svg" alt="AutoEncoderToolkit logo"/><img class="docs-dark-only" src="../assets/logo-dark.svg" alt="AutoEncoderToolkit logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AutoEncoderToolkit</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><a class="tocitem" href="../encoders/">Encoders &amp; Decoders</a></li><li><a class="tocitem" href="../layers/">Custom Layers</a></li><li><a class="tocitem" href="../ae/">Deterministic Autoencoders</a></li><li><a class="tocitem" href="../vae/">VAE / β-VAE</a></li><li><a class="tocitem" href="../mmdvae/">MMD-VAE (InfoVAE)</a></li><li><a class="tocitem" href="../infomaxvae/">InfoMax-VAE</a></li><li class="is-active"><a class="tocitem" href>HVAE</a><ul class="internal"><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li><li><a class="tocitem" href="#HVAEstruct"><span><code>HVAE</code> struct</span></a></li><li><a class="tocitem" href="#Forward-pass"><span>Forward pass</span></a></li><li><a class="tocitem" href="#Loss-function"><span>Loss function</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#gradpotenergy"><span>Computing the gradient of the potential energy</span></a></li><li><a class="tocitem" href="#Other-Functions"><span>Other Functions</span></a></li></ul></li><li><a class="tocitem" href="../rhvae/">RHVAE</a></li><li><a class="tocitem" href="../diffgeo/">Differential Geometry</a></li><li><a class="tocitem" href="../utils/">Utilities</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>HVAE</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>HVAE</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="HVAEsmodule"><a class="docs-heading-anchor" href="#HVAEsmodule">Hamiltonian Variational Autoencoder</a><a id="HVAEsmodule-1"></a><a class="docs-heading-anchor-permalink" href="#HVAEsmodule" title="Permalink"></a></h1><p>The Hamiltonian Variational Autoencoder (HVAE) is a variant of the Variational autoencoder (VAE) that uses Hamiltonian dynamics to improve the sampling of the latent space representation. HVAE combines ideas from Hamiltonian Monte Carlo, annealed importance sampling, and variational inference to improve the latent space representation of the VAE.</p><p>For the implementation of the HVAE in <code>AutoEncoderToolkit.jl</code>, the <a href="#HVAEstruct"><code>HVAE</code></a> struct inherits directly from the <a href="../vae/#VAEstruct"><code>VAE</code></a> struct and adds the necessary functions to compute the Hamiltonian dynamics steps as part of the training protocol. An <code>HVAE</code> object is created by simply passing a <code>VAE</code> object to the constructor. This way, we can use <code>Julia</code>s multiple dispatch to extend the functionality of the <code>VAE</code> object without having to redefine the entire structure.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>HVAEs require the computation of nested gradients. This means that the AutoDiff framework must differentiate a function of an already AutoDiff differentiated function. This is known to be problematic for <code>Julia</code>&#39;s AutoDiff backends. See <a href="#gradpotenergy">details below</a> to understand how to we circumvent this problem.</p></div></div><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><blockquote><p>Caterini, A. L., Doucet, A. &amp; Sejdinovic, D. Hamiltonian Variational Auto-Encoder. 11 (2018).</p></blockquote><h2 id="HVAEstruct"><a class="docs-heading-anchor" href="#HVAEstruct"><code>HVAE</code> struct</a><a id="HVAEstruct-1"></a><a class="docs-heading-anchor-permalink" href="#HVAEstruct" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.HVAE" href="#AutoEncoderToolkit.HVAEs.HVAE"><code>AutoEncoderToolkit.HVAEs.HVAE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct HVAE{
    V&lt;:VAE{&lt;:AbstractVariationalEncoder,&lt;:AbstractVariationalDecoder}
} &lt;: AbstractVariationalAutoEncoder</code></pre><p>Hamiltonian Variational Autoencoder (HVAE) model defined for <code>Flux.jl</code>.</p><p><strong>Fields</strong></p><ul><li><code>vae::V</code>: A Variational Autoencoder (VAE) model that forms the basis of the   HVAE. <code>V</code> is a subtype of <code>VAE</code> with a specific <code>AbstractVariationalEncoder</code>   and <code>AbstractVariationalDecoder</code>.</li></ul><p>An HVAE is a type of Variational Autoencoder (VAE) that uses Hamiltonian Monte Carlo (HMC) to sample from the posterior distribution in the latent space. The VAE&#39;s encoder compresses the input into a low-dimensional probabilistic representation q(z|x). The VAE&#39;s decoder tries to reconstruct the original input from a sampled point in the latent space p(x|z). </p><p>The HMC sampling in the latent space allows the HVAE to better capture complex posterior distributions compared to a standard VAE, which assumes a simple Gaussian posterior. This can lead to more accurate reconstructions and better disentanglement of latent variables.</p></div></section></article><h2 id="Forward-pass"><a class="docs-heading-anchor" href="#Forward-pass">Forward pass</a><a id="Forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-pass" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.HVAE-Tuple{AbstractArray}" href="#AutoEncoderToolkit.HVAEs.HVAE-Tuple{AbstractArray}"><code>AutoEncoderToolkit.HVAEs.HVAE</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(hvae::HVAE{VAE{E,D}})(
    x::AbstractArray;
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    K::Int=3,
    βₒ::Number=0.3f0,
    ∇U_kwargs::Union{Dict,NamedTuple}=(
            reconstruction_loglikelihood=reconstruction_loglikelihood,
            latent_logprior=spherical_logprior,
    ),
    tempering_schedule::Function=quadratic_tempering,
    latent::Bool=false,
) where {E&lt;:AbstractGaussianLogEncoder,D&lt;:AbstractVariationalDecoder}</code></pre><p>Run the Hamiltonian Variational Autoencoder (HVAE) on the given input.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: The input to the HVAE. If <code>Vector</code>, it represents a single data point. If <code>Array</code>, the last dimension must contain each of the data points.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}=0.0001</code>: The step size for the leapfrog steps in the HMC part of the HVAE. If it is a scalar, the same step size is used for all dimensions. If it is an array, each element corresponds to the step size for a specific dimension.</li><li><code>K::Int=3</code>: The number of leapfrog steps to perform in the Hamiltonian Monte Carlo (HMC) part of the HVAE.</li><li><code>βₒ::Number=0.3f0</code>: The initial inverse temperature for the tempering schedule.</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: Additional keyword arguments to be passed to the <code>∇potential_energy</code> function. Default is a NamedTuple with <code>reconstruction_loglikelihood</code> and <code>latent_logprior</code>.</li><li><code>tempering_schedule::Function=quadratic_tempering</code>: The function to compute   the tempering schedule in the HVAE.</li><li><code>latent::Bool=false</code>: If <code>true</code>, the function returns a NamedTuple containing the outputs of the encoder and decoder, and the final state of the phase space after the leapfrog and tempering steps. If <code>false</code>, the function only returns the output of the decoder.</li></ul><p><strong>Returns</strong></p><p>If <code>latent=true</code>, the function returns a NamedTuple with the following fields:</p><ul><li><code>encoder</code>: The outputs of the encoder.</li><li><code>decoder</code>: The output of the decoder.</li><li><code>phase_space</code>: The final state of the phase space after the leapfrog and   tempering steps.</li></ul><p>If <code>latent=false</code>, the function only returns the output of the decoder.</p><p><strong>Description</strong></p><p>This function runs the HVAE on the given input. It first passes the input through the encoder to obtain the mean and log standard deviation of the latent space. It then uses the reparameterization trick to sample from the latent space. After that, it performs the leapfrog and tempering steps to refine the sample from the latent space. Finally, it passes the refined sample through the decoder to obtain the output.</p><p><strong>Notes</strong></p><p>Ensure that the dimensions of <code>x</code> match the input dimensions of the HVAE, and that the dimensions of <code>ϵ</code> match the dimensions of the latent space.</p></div></section></article><h2 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.loss" href="#AutoEncoderToolkit.HVAEs.loss"><code>AutoEncoderToolkit.HVAEs.loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">loss(
    hvae::HVAE,
    x::AbstractArray;
    K::Int=3,
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    βₒ::Number=0.3f0,
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=reconstruction_loglikelihood,
        latent_logprior=spherical_logprior,
    ),
    tempering_schedule::Function=quadratic_tempering,
    reg_function::Union{Function,Nothing}=nothing,
    reg_kwargs::Union{NamedTuple,Dict}=Dict(),
    reg_strength::Float32=1.0f0,
    logp_prefactor::AbstractArray=ones(Float32, 3),
    logq_prefactor::AbstractArray=ones(Float32, 3),
)</code></pre><p>Compute the loss for a Hamiltonian Variational Autoencoder (HVAE).</p><p><strong>Arguments</strong></p><ul><li><code>hvae::HVAE</code>: The HVAE used to encode the input data and decode the latent space.</li><li><code>x::AbstractArray</code>: Input data to the HVAE encoder. The last dimension is taken as having each of the samples in a batch.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>K::Int</code>: The number of HMC steps (default is 3).</li><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}</code>: The step size for the leapfrog integrator (default is 0.001).</li><li><code>βₒ::Number</code>: The initial inverse temperature (default is 0.3).</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: Additional keyword arguments to be passed to the <code>∇potential_energy</code> function.</li><li><code>tempering_schedule::Function</code>: The tempering schedule function used in the HMC (default is <code>quadratic_tempering</code>).</li><li><code>reg_function::Union{Function, Nothing}=nothing</code>: A function that computes the regularization term based on the VAE outputs. This function must take as input the VAE outputs and the keyword arguments provided in <code>reg_kwargs</code>.</li><li><code>reg_kwargs::Union{NamedTuple,Dict}=Dict()</code>: Keyword arguments to pass to the regularization function.</li><li><code>reg_strength::Float32=1.0f0</code>: The strength of the regularization term.</li><li><code>logp_prefactor::AbstractArray</code>: A 3-element array to scale the log likelihood, log prior of the latent variables, and log prior of the momentum variables. Default is an array of ones.</li><li><code>logq_prefactor::AbstractArray</code>: A 3-element array to scale the log posterior of the initial latent variables, log prior of the initial momentum variables, and the tempering Jacobian term. Default is an array of ones.</li></ul><p><strong>Returns</strong></p><ul><li>The computed loss.</li></ul></div></section><section><div><pre><code class="language-julia hljs">loss(
    hvae::HVAE,
    x_in::AbstractArray,
    x_out::AbstractArray;
    K::Int=3,
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    βₒ::Number=0.3f0,
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=reconstruction_loglikelihood,
        latent_logprior=spherical_logprior,
    ),
    tempering_schedule::Function=quadratic_tempering,
    reg_function::Union{Function,Nothing}=nothing,
    reg_kwargs::Union{NamedTuple,Dict}=Dict(),
    reg_strength::Float32=1.0f0,
    logp_prefactor::AbstractArray=ones(Float32, 3),
    logq_prefactor::AbstractArray=ones(Float32, 3),
)</code></pre><p>Compute the loss for a Hamiltonian Variational Autoencoder (HVAE).</p><p><strong>Arguments</strong></p><ul><li><code>hvae::HVAE</code>: The HVAE used to encode the input data and decode the latent space.</li><li><code>x_in::AbstractArray</code>: Input data to the HVAE encoder. The last dimension is taken as having each of the samples in a batch.</li><li><code>x_out::AbstractArray</code>: The data against which the reconstruction is compared. If <code>Array</code>, the last dimension must contain each of the data points.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>K::Int</code>: The number of HMC steps (default is 3).</li><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}</code>: The step size for the leapfrog integrator (default is 0.001).</li><li><code>βₒ::Number</code>: The initial inverse temperature (default is 0.3).</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: Additional keyword arguments to be passed to the <code>∇potential_energy</code> function.</li><li><code>tempering_schedule::Function</code>: The tempering schedule function used in the HMC (default is <code>quadratic_tempering</code>).</li><li><code>reg_function::Union{Function, Nothing}=nothing</code>: A function that computes the regularization term based on the VAE outputs. This function must take as input the VAE outputs and the keyword arguments provided in <code>reg_kwargs</code>.</li><li><code>reg_kwargs::Union{NamedTuple,Dict}=Dict()</code>: Keyword arguments to pass to the regularization function.</li><li><code>reg_strength::Float32=1.0f0</code>: The strength of the regularization term.</li><li><code>logp_prefactor::AbstractArray</code>: A 3-element array to scale the log likelihood, log prior of the latent variables, and log prior of the momentum variables. Default is an array of ones.</li><li><code>logq_prefactor::AbstractArray</code>: A 3-element array to scale the log posterior of the initial latent variables, log prior of the initial momentum variables, and the tempering Jacobian term. Default is an array of ones.</li></ul><p><strong>Returns</strong></p><ul><li>The computed loss.</li></ul></div></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.train!" href="#AutoEncoderToolkit.HVAEs.train!"><code>AutoEncoderToolkit.HVAEs.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train!(
    hvae::HVAE, 
    x::AbstractArray, 
    opt::NamedTuple; 
    loss_function::Function=loss, 
    loss_kwargs::Union{NamedTuple,Dict}=Dict(),
    verbose::Bool=false,
    loss_return::Bool=false,
)</code></pre><p>Customized training function to update parameters of a Hamiltonian Variational Autoencoder given a specified loss function.</p><p><strong>Arguments</strong></p><ul><li><code>hvae::HVAE</code>: A struct containing the elements of a Hamiltonian Variational Autoencoder.</li><li><code>x::AbstractArray</code>: Input data to the HVAE encoder. The last dimension is taken as having each of the samples in a batch.</li><li><code>opt::NamedTuple</code>: State of the optimizer for updating parameters. Typically initialized using <code>Flux.Optimisers.update!</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>loss_function::Function=loss</code>: The loss function used for training. It should accept the HVAE model, data <code>x</code>, and keyword arguments in that order.</li><li><code>loss_kwargs::Dict=Dict()</code>: Arguments for the loss function. These might include parameters like <code>K</code>, <code>ϵ</code>, <code>βₒ</code>, <code>steps</code>, <code>∇H</code>, <code>∇H_kwargs</code>, <code>tempering_schedule</code>, <code>reg_function</code>, <code>reg_kwargs</code>, <code>reg_strength</code>, depending on the specific loss function in use.</li><li><code>verbose::Bool=false</code>: Whether to print the loss at each iteration.</li><li><code>loss_return::Bool=false</code>: Whether to return the loss at each iteration.</li></ul><p><strong>Description</strong></p><p>Trains the HVAE by:</p><ol><li>Computing the gradient of the loss w.r.t the HVAE parameters.</li><li>Updating the HVAE parameters using the optimizer.</li><li>Updating the metric parameters.</li></ol></div></section><section><div><pre><code class="language-julia hljs">train!(
    hvae::HVAE, 
    x_in::AbstractArray,
    x_out::AbstractArray,
    opt::NamedTuple; 
    loss_function::Function=loss, 
    loss_kwargs::Union{NamedTuple,Dict}=Dict(),
    verbose::Bool=false,
    loss_return::Bool=false,
)</code></pre><p>Customized training function to update parameters of a Hamiltonian Variational Autoencoder given a specified loss function.</p><p><strong>Arguments</strong></p><ul><li><code>hvae::HVAE</code>: A struct containing the elements of a Hamiltonian Variational Autoencoder.</li><li><code>x_in::AbstractArray</code>: Input data to the HVAE encoder. The last dimension is taken as having each of the samples in a batch.</li><li><code>x_out::AbstractArray</code>: Target data to compute the reconstruction error. The last dimension is taken as having each of the samples in a batch.</li><li><code>opt::NamedTuple</code>: State of the optimizer for updating parameters. Typically initialized using <code>Flux.Optimisers.update!</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>loss_function::Function=loss</code>: The loss function used for training. It should accept the HVAE model, data <code>x</code>, and keyword arguments in that order.</li><li><code>loss_kwargs::Dict=Dict()</code>: Arguments for the loss function. These might include parameters like <code>K</code>, <code>ϵ</code>, <code>βₒ</code>, <code>steps</code>, <code>∇H</code>, <code>∇H_kwargs</code>, <code>tempering_schedule</code>, <code>reg_function</code>, <code>reg_kwargs</code>, <code>reg_strength</code>, depending on the specific loss function in use.</li><li><code>verbose::Bool=false</code>: Whether to print the loss at each iteration.</li><li><code>loss_return::Bool=false</code>: Whether to return the loss at each iteration.</li></ul><p><strong>Description</strong></p><p>Trains the HVAE by:</p><ol><li>Computing the gradient of the loss w.r.t the HVAE parameters.</li><li>Updating the HVAE parameters using the optimizer.</li><li>Updating the metric parameters.</li></ol></div></section></article><h2 id="gradpotenergy"><a class="docs-heading-anchor" href="#gradpotenergy">Computing the gradient of the potential energy</a><a id="gradpotenergy-1"></a><a class="docs-heading-anchor-permalink" href="#gradpotenergy" title="Permalink"></a></h2><p>One of the crucial components in the training of the HVAE is the computation of the gradient of the potential energy <span>$\nabla U$</span> with respect to the latent space representation. This gradient is used in the leapfrog steps of the Hamiltonian dynamics. When training the HVAE, we need to backpropagate through the leapfrog steps to update the parameters of the neural network. This requires computing a gradient of a function of the gradient of the potential energy, i.e., nested gradients. <code>Zygote.jl</code> the main AutoDiff backend in <code>Flux.jl</code> <a href="https://discourse.julialang.org/t/is-it-possible-to-do-nested-ad-elegantly-in-julia-pinns/98888">famously struggle</a> with these types of computations. Specifically, <code>Zygote.jl</code> does not support <code>Zygote</code> over <code>Zygote</code> differentiation (meaning differentiating a function of something previously differentiated with <code>Zygote</code> using <code>Zygote</code>), or <code>Zygote</code> over <code>ForwardDiff</code> (meaning differentiating a function of something differentiated with <code>ForwardDiff</code> using <code>Zygote</code>).</p><p>With this, we are left with a couple of options to compute the gradient of the potential energy:</p><ul><li>Use finite differences to approximate the gradient of the potential energy.</li><li>Use the relatively new <a href="https://github.com/JuliaDiff/TaylorDiff.jl/tree/main"><code>TaylorDiff.jl</code></a> AutoDiff backend to compute the gradient of the potential energy. This backend is composable with <code>Zygote.jl</code>, so we can, in principle, do <code>Zygote</code> over <code>TaylorDiff</code> differentiation.</li></ul><p>The second option would be preferred, as the gradients computed with <code>TaylorDiff</code> are much more accurate than the ones computed with finite differences. However, there are two problems with this approach:</p><ol><li>The <code>TaylorDiff</code> nested gradient capability stopped working with <code>Julia ≥  1.10</code>, as discussed in  <a href="https://github.com/JuliaDiff/TaylorDiff.jl/issues/70">#70</a>.</li><li>Even for <code>Julia &lt; 1.10</code>, we could not get <code>TaylorDiff</code> to work on <code>CUDA</code>  devices. (PRs are welcome!)</li></ol><p>With these limitations in mind, we have implemented the gradient of the potential using both finite differences and <code>TaylorDiff</code>. The user can choose which method to use by setting the <code>adtype</code> keyword argument in the <code>∇U_kwargs</code> in the <code>loss</code> function to either <code>:finite</code> or <code>:TaylorDiff</code>. This means that for the <code>train!</code> function, the user can pass <code>loss_kwargs</code> that looks like this:</p><pre><code class="language-julia hljs"># Define the autodiff backend to use
loss_kwargs = Dict(
    :∇U_kwargs =&gt; Dict(
        :adtype =&gt; :finite
    )
)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Although verbose, the nested dictionaries help to keep everything organized. (PRs with better design ideas are welcome!)</p></div></div><p>The default both for <code>cpu</code> and <code>gpu</code> devices is <code>:finite</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.∇potential_energy_finite" href="#AutoEncoderToolkit.HVAEs.∇potential_energy_finite"><code>AutoEncoderToolkit.HVAEs.∇potential_energy_finite</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">∇potential_energy_finite(
    x::AbstractArray,
    z::AbstractVecOrMat,
    decoder::AbstractVariationalDecoder,
    decoder_output::NamedTuple;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior,
    fdtype::Symbol=:central
)</code></pre><p>Compute the gradient of the potential energy of a Hamiltonian Variational Autoencoder (HVAE) with respect to the latent variables <code>z</code> using finite difference method. This function returns the gradient of the potential energy computed for given data <code>x</code> and latent variable <code>z</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>decoder::AbstractVariationalDecoder</code>: A decoder that maps the latent variables to the data space.</li><li><code>decoder_output::NamedTuple</code>: The output of the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input an <code>AbstractVariationalDecoder</code> struct, as second input an array <code>x</code> representing the data, and as third input a vector or matrix <code>z</code> representing the latent variable. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector or matrix <code>z</code> representing the latent variable. Default is <code>spherical_logprior</code>.  </li><li><code>fdtype::Symbol=:central</code>: A symbol representing the type of finite difference method to use. Default is <code>:central</code>, but it can also be <code>:forward</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>gradient</code>: The computed gradient of the potential energy for the given input <code>x</code> and latent variable <code>z</code>.</li></ul></div></section><section><div><pre><code class="language-julia hljs">∇potential_energy_finite(
    x::AbstractArray,
    z::AbstractVecOrMat,
    hvae::HVAE;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior,
    fdtype::Symbol=:central
)</code></pre><p>Compute the gradient of the potential energy of a Hamiltonian Variational Autoencoder (HVAE) with respect to the latent variables <code>z</code> using finite difference method. This function returns the gradient of the potential energy computed for given data <code>x</code> and latent variable <code>z</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>hvae::HVAE</code>: An HVAE model that contains a decoder which maps the latent variables to the data space.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input an array <code>x</code> representing the data, as second input a vector or matrix <code>z</code> representing the latent variable, and as third input a decoder. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector or matrix <code>z</code> representing the latent variable. Default is <code>spherical_logprior</code>.  </li><li><code>fdtype::Symbol=:central</code>: A symbol representing the type of finite difference method to use. Default is <code>:central</code>, but it can also be <code>:forward</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>gradient</code>: The computed gradient of the potential energy for the given input <code>x</code> and latent variable <code>z</code>.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.∇potential_energy_TaylorDiff" href="#AutoEncoderToolkit.HVAEs.∇potential_energy_TaylorDiff"><code>AutoEncoderToolkit.HVAEs.∇potential_energy_TaylorDiff</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">∇potential_energy_TaylorDiff(
    x::AbstractArray,
    z::AbstractVecOrMat,
    hvae::HVAE;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior,
)</code></pre><p>Compute the gradient of the potential energy of a Hamiltonian Variational Autoencoder (HVAE) with respect to the latent variables <code>z</code> using Taylor series differentiation. This function returns the gradient of the potential energy computed for given data <code>x</code> and latent variable <code>z</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>hvae::HVAE</code>: An HVAE model that contains a decoder which maps the latent variables to the data space.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input an array <code>x</code> representing the data, as second input a vector or matrix <code>z</code> representing the latent variable, and as third input a decoder. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector or matrix <code>z</code> representing the latent variable.  Default is <code>spherical_logprior</code>.  </li></ul><p><strong>Returns</strong></p><ul><li><code>gradient</code>: The computed gradient of the potential energy for the given input   <code>x</code> and latent variable <code>z</code>.</li></ul></div></section><section><div><pre><code class="language-julia hljs">∇potential_energy_TaylorDiff(
    x::AbstractArray,
    z::AbstractVecOrMat,
    hvae::HVAE;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior,
)</code></pre><p>Compute the gradient of the potential energy of a Hamiltonian Variational Autoencoder (HVAE) with respect to the latent variables <code>z</code> using Taylor series differentiation. This function returns the gradient of the potential energy computed for given data <code>x</code> and latent variable <code>z</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>hvae::HVAE</code>: An HVAE model that contains a decoder which maps the latent variables to the data space.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input an array <code>x</code> representing the data, as second input a vector or matrix <code>z</code> representing the latent variable, and as third input a decoder. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector or matrix <code>z</code> representing the latent variable.  Default is <code>spherical_logprior</code>.  </li></ul><p><strong>Returns</strong></p><ul><li><code>gradient</code>: The computed gradient of the potential energy for the given input   <code>x</code> and latent variable <code>z</code>.</li></ul></div></section></article><h2 id="Other-Functions"><a class="docs-heading-anchor" href="#Other-Functions">Other Functions</a><a id="Other-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Other-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.potential_energy" href="#AutoEncoderToolkit.HVAEs.potential_energy"><code>AutoEncoderToolkit.HVAEs.potential_energy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">potential_energy(
    x::AbstractVector,
    z::AbstractVector,
    decoder::AbstractVariationalDecoder,
    decoder_output::NamedTuple;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior
)</code></pre><p>Compute the potential energy of a Hamiltonian Variational Autoencoder (HVAE). In the context of Hamiltonian Monte Carlo (HMC), the potential energy is defined as the negative log-posterior. This function computes the potential energy for given data <code>x</code> and latent variable <code>z</code>. It does this by computing the log-likelihood of <code>x</code> under the distribution defined by <code>reconstruction_loglikelihood(x, z, decoder, decoder_output)</code>, and the log-prior of <code>z</code> under the <code>latent_logprior</code> distribution. The potential energy is then computed as:</p><pre><code class="nohighlight hljs">    U(x, z) = -log p(x | z) - log p(z)</code></pre><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>decoder::AbstractVariationalDecoder</code>: A decoder that maps the latent variables to the data space.</li><li><code>decoder_output::NamedTuple</code>: The output of the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input a vector <code>x</code> representing the data, as second input a vector <code>z</code> representing the latent variable, as third input a decoder, and as fourth input a NamedTuple representing the decoder output. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector <code>z</code> representing the latent variable. Default is <code>spherical_logprior</code>.  </li></ul><p><strong>Returns</strong></p><ul><li><code>energy</code>: The computed potential energy for the given input <code>x</code> and latent variable <code>z</code>.</li></ul></div></section><section><div><pre><code class="language-julia hljs">potential_energy(
    x::AbstractArray,
    z::AbstractVecOrMat,
    hvae::HVAE;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior
)</code></pre><p>Compute the potential energy of a Hamiltonian Variational Autoencoder (HVAE). In the context of Hamiltonian Monte Carlo (HMC), the potential energy is defined as the negative log-posterior. This function computes the potential energy for given data <code>x</code> and latent variable <code>z</code>. It does this by computing the log-likelihood of <code>x</code> under the distribution defined by <code>reconstruction_loglikelihood(x, z, hvae.vae.decoder, decoder_output)</code>, and the log-prior of <code>z</code> under the <code>latent_logprior</code> distribution. The potential energy is then computed as:</p><pre><code class="nohighlight hljs">            U(x, z) = -log p(x | z) - log p(z)</code></pre><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>hvae::HVAE</code>: A Hamiltonian Variational Autoencoder that contains the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input an array <code>x</code> representing the data, as second input a vector or matrix <code>z</code> representing the latent variable, as third input a decoder, and as fourth input a NamedTuple representing the decoder output. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector or matrix <code>z</code> representing the latent variable.  Default is <code>spherical_logprior</code>.  </li></ul><p><strong>Returns</strong></p><ul><li><code>energy</code>: The computed potential energy for the given input <code>x</code> and latent   variable <code>z</code>.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.∇potential_energy" href="#AutoEncoderToolkit.HVAEs.∇potential_energy"><code>AutoEncoderToolkit.HVAEs.∇potential_energy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">∇potential_energy(
    x::AbstractArray,
    z::AbstractVecOrMat,
    decoder::AbstractVariationalDecoder,
    decoder_output::NamedTuple;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior,
    adtype::Union{Symbol,Nothing}=nothing,
    adkwargs::Union{NamedTuple,Dict}=Dict(),
)</code></pre><p>Compute the gradient of the potential energy of a Hamiltonian Variational Autoencoder (HVAE) with respect to the latent variables <code>z</code> using the specified automatic differentiation method. This function returns the gradient of the potential energy computed for given data <code>x</code> and latent variable <code>z</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>decoder::AbstractVariationalDecoder</code>: A decoder that maps the latent variables to the data space.</li><li><code>decoder_output::NamedTuple</code>: The output of the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input an <code>AbstractVariationalDecoder</code> struct, as second input an array <code>x</code> representing the data, and as third input a vector or matrix <code>z</code> representing the latent variable. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector or matrix <code>z</code> representing the latent variable. Default is <code>spherical_logprior</code>.  </li><li><code>adtype::Symbol</code>=:finite<code>: The type of automatic differentiation method to use. Must be</code>:finite<code>or</code>:TaylorDiff<code>. Default is</code>:finite`.</li><li><code>adkwargs::Union{NamedTuple,Dict}=Dict()</code>: Additional keyword arguments to pass to the automatic differentiation method.</li></ul><p><strong>Returns</strong></p><ul><li><code>gradient</code>: The computed gradient of the potential energy for the given input   <code>x</code> and latent variable <code>z</code>.</li></ul></div></section><section><div><pre><code class="language-julia hljs">∇potential_energy(
    x::AbstractArray,
    z::AbstractVecOrMat,
    hvae::HVAE;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    latent_logprior::Function=spherical_logprior,
    adtype::Union{Symbol,Nothing}=nothing,
    adkwargs::Union{NamedTuple,Dict}=Dict(),
)</code></pre><p>Compute the gradient of the potential energy of a Hamiltonian Variational Autoencoder (HVAE) with respect to the latent variables <code>z</code> using the specified automatic differentiation method. This function returns the gradient of the potential energy computed for given data <code>x</code> and latent variable <code>z</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: An array representing the input data. The last dimension corresponds to different data points.</li><li><code>z::AbstractVecOrMat</code>: A latent variable encoding of the input data. If a matrix, each column corresponds to a different data point.</li><li><code>hvae::HVAE</code>: An HVAE model that contains a decoder which maps the latent variables to the data space.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function representing the log-likelihood function used by the decoder. The function must take as first input an array <code>x</code> representing the data, as second input a vector or matrix <code>z</code> representing the latent variable, and as third input a decoder. Default is <code>decoder_loglikelihood</code>.</li><li><code>latent_logprior::Function=spherical_logprior</code>: A function representing the log-prior distribution used in the autoencoder. The function must take as single input a vector or matrix <code>z</code> representing the latent variable.  Default is <code>spherical_logprior</code>.  <ul><li><code>adtype::Symbol</code>=:finite`: The type of automatic differentiation method to</li></ul>use. Must be <code>:finite</code> or <code>:TaylorDiff</code>. Default is <code>:finite</code>.</li><li><code>adkwargs::Union{NamedTuple,Dict}=Dict()</code>: Additional keyword arguments to pass to the automatic differentiation method.</li></ul><p><strong>Returns</strong></p><ul><li><code>gradient</code>: The computed gradient of the potential energy for the given input <code>x</code> and latent variable <code>z</code>.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.leapfrog_step" href="#AutoEncoderToolkit.HVAEs.leapfrog_step"><code>AutoEncoderToolkit.HVAEs.leapfrog_step</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">leapfrog_step(
    x::AbstractArray,
    z::AbstractVecOrMat,
    ρ::AbstractVecOrMat,
    decoder::AbstractVariationalDecoder,
    decoder_output::NamedTuple;
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=reconstruction_loglikelihood,
        latent_logprior=spherical_logprior,
    )
)</code></pre><p>Perform a full step of the leapfrog integrator for Hamiltonian dynamics.</p><p>The leapfrog integrator is a numerical integration scheme used to simulate Hamiltonian dynamics. It consists of three steps:</p><ol><li><p>Half update of the momentum variable: </p><pre><code class="nohighlight hljs"> ρ(t + ϵ/2) = ρ(t) - 0.5 * ϵ * ∇z_U(z(t), ρ(t + ϵ/2)).</code></pre></li><li><p>Full update of the position variable: </p><pre><code class="nohighlight hljs"> z(t + ϵ) = z(t) + ϵ * ρ(t + ϵ/2).</code></pre></li><li><p>Half update of the momentum variable: </p><pre><code class="nohighlight hljs"> ρ(t + ϵ) = ρ(t + ϵ/2) - 0.5 * ϵ * ∇z_U(z(t + ϵ), ρ(t + ϵ/2)).</code></pre></li></ol><p>This function performs these three steps in sequence.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: The point in the data space. This does not necessarily need to be a vector. Array inputs are supported. The last dimension is assumed to have each of the data points.</li><li><code>z::AbstractVecOrMat</code>: The point in the latent space. If matrix, each column represents a point in the latent space.</li><li><code>ρ::AbstractVecOrMat</code>: The momentum. If matrix, each column represents a momentum vector.</li><li><code>decoder::AbstractVariationalDecoder</code>: The decoder instance.</li><li><code>decoder_output::NamedTuple</code>: The output of the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4)</code>: The step size. Default is 0.0001.</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: The keyword arguments for <code>∇potential_energy</code>.  Default is a tuple with <code>reconstruction_loglikelihood</code> and <code>latent_logprior</code>.</li></ul><p><strong>Returns</strong></p><p>A tuple <code>(z̄, ρ̄, decoder_output_z̄)</code> representing the updated position and momentum after performing the full leapfrog step as well as the decoder output of the updated position.</p></div></section><section><div><pre><code class="language-julia hljs">leapfrog_step(
    x::AbstractArray,
    z::AbstractVecOrMat,
    ρ::AbstractVecOrMat,
    hvae::HVAE;
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=reconstruction_loglikelihood,
        latent_logprior=spherical_logprior,
    )
)</code></pre><p>Perform a full step of the leapfrog integrator for Hamiltonian dynamics.</p><p>The leapfrog integrator is a numerical integration scheme used to simulate Hamiltonian dynamics. It consists of three steps:</p><ol><li><p>Half update of the momentum variable: </p><pre><code class="nohighlight hljs"> ρ(t + ϵ/2) = ρ(t) - 0.5 * ϵ * ∇z_U(z(t), ρ(t + ϵ/2)).</code></pre></li><li><p>Full update of the position variable: </p><pre><code class="nohighlight hljs"> z(t + ϵ) = z(t) + ϵ * ρ(t + ϵ/2).</code></pre></li><li><p>Half update of the momentum variable: </p><pre><code class="nohighlight hljs"> ρ(t + ϵ) = ρ(t + ϵ/2) - 0.5 * ϵ * ∇z_U(z(t + ϵ), ρ(t + ϵ/2)).</code></pre></li></ol><p>This function performs these three steps in sequence.</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: The point in the data space. This does not necessarily need to be a vector. Array inputs are supported. The last dimension is assumed to have each of the data points.</li><li><code>z::AbstractVecOrMat</code>: The point in the latent space. If matrix, each column represents a point in the latent space.</li><li><code>ρ::AbstractVecOrMat</code>: The momentum. If matrix, each column represents a momentum vector.</li><li><code>hvae::HVAE</code>: An HVAE model that contains the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4)</code>: The step size. Default is 0.0001.</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: The keyword arguments for <code>∇potential_energy</code>.  Default is a tuple with <code>reconstruction_loglikelihood</code> and <code>latent_logprior</code>.</li></ul><p><strong>Returns</strong></p><p>A tuple <code>(z̄, ρ̄, decoder_output_z̄)</code> representing the updated position and momentum after performing the full leapfrog step as well as the decoder output of the updated position.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.quadratic_tempering" href="#AutoEncoderToolkit.HVAEs.quadratic_tempering"><code>AutoEncoderToolkit.HVAEs.quadratic_tempering</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">quadratic_tempering(βₒ::AbstractFloat, k::Int, K::Int)</code></pre><p>Compute the inverse temperature <code>βₖ</code> at a given stage <code>k</code> of a tempering schedule with <code>K</code> total stages, using a quadratic tempering scheme. </p><p>Tempering is a technique used in sampling algorithms to improve mixing and convergence. It involves running parallel chains of the algorithm at different &quot;temperatures&quot;, and swapping states between the chains. The &quot;temperature&quot; of a chain is controlled by an inverse temperature parameter <code>β</code>, which is varied according to a tempering schedule. </p><p>In a quadratic tempering schedule, the inverse temperature <code>βₖ</code> at stage <code>k</code> is computed as the square of the quantity <code>((1 - 1 / √(βₒ)) * (k / K)^2 + 1 / √(βₒ))</code>, where <code>βₒ</code> is the initial inverse temperature. This schedule starts at <code>βₒ</code> when <code>k = 0</code>, and increases quadratically as <code>k</code> increases, reaching 1 when <code>k = K</code>.</p><p><strong>Arguments</strong></p><ul><li><code>βₒ::AbstractFloat</code>: The initial inverse temperature.</li><li><code>k::Int</code>: The current stage of the tempering schedule.</li><li><code>K::Int</code>: The total number of stages in the tempering schedule.</li></ul><p><strong>Returns</strong></p><ul><li><code>βₖ::AbstractFloat</code>: The inverse temperature at stage <code>k</code>.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.null_tempering" href="#AutoEncoderToolkit.HVAEs.null_tempering"><code>AutoEncoderToolkit.HVAEs.null_tempering</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">    null_tempering(βₒ::T, k::Int, K::Int) where {T&lt;:AbstractFloat}</code></pre><p>Return the initial inverse temperature <code>βₒ</code>. This function is used in the context of tempered Hamiltonian Monte Carlo (HMC) methods, where tempering involves running HMC at different &quot;temperatures&quot; to improve mixing and convergence. </p><p>In this case, <code>null_tempering</code> is a simple tempering schedule that does not actually change the temperature—it always returns the initial inverse temperature <code>βₒ</code>. This can be useful as a default or placeholder tempering schedule.</p><p><strong>Arguments</strong></p><ul><li><code>βₒ::AbstractFloat</code>: The initial inverse temperature. </li><li><code>k::Int</code>: The current step in the tempering schedule. Not used in this   function, but included for compatibility with other tempering schedules.</li><li><code>K::Int</code>: The total number of steps in the tempering schedule. Not used in   this function, but included for compatibility with other tempering   schedules.</li></ul><p><strong>Returns</strong></p><ul><li><code>β::T</code>: The inverse temperature for the current step, which is always <code>βₒ</code> in this case.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">βₒ = 0.5
k = 1
K = 10
β = null_tempering(βₒ, k, K)  # β will be 0.5</code></pre></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.leapfrog_tempering_step" href="#AutoEncoderToolkit.HVAEs.leapfrog_tempering_step"><code>AutoEncoderToolkit.HVAEs.leapfrog_tempering_step</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">leapfrog_tempering_step(
    x::AbstractArray,
    zₒ::AbstractVecOrMat,
    decoder::AbstractVariationalDecoder,
    decoder_output::NamedTuple;
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    K::Int=3,
    βₒ::Number=0.3f0,
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=reconstruction_loglikelihood,
        latent_logprior=spherical_logprior,
    ),
    tempering_schedule::Function=quadratic_tempering,
)</code></pre><p>Combines the leapfrog and tempering steps into a single function for the Hamiltonian Variational Autoencoder (HVAE).</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: The data to be processed. If <code>Array</code>, the last dimension must be of size 1.</li><li><code>zₒ::AbstractVecOrMat</code>: The initial latent variable. </li><li><code>decoder::AbstractVariationalDecoder</code>: The decoder of the HVAE model.</li><li><code>decoder_output::NamedTuple</code>: The output of the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}</code>: The step size for the leapfrog steps in the HMC algorithm. This can be a scalar or an array. Default is 0.0001.  </li><li><code>K::Int</code>: The number of leapfrog steps to perform in the Hamiltonian Monte Carlo (HMC) algorithm. Default is 3.</li><li><code>βₒ::Number</code>: The initial inverse temperature for the tempering schedule. Default is 0.3f0.</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: Additional keyword arguments to be passed to the <code>∇potential_energy</code> function. Default is a NamedTuple with <code>reconstruction_loglikelihood</code> and <code>latent_logprior</code>.</li><li><code>tempering_schedule::Function</code>: The function to compute the inverse temperature at each step in the HMC algorithm. Defaults to <code>quadratic_tempering</code>. This function must take three arguments: First, <code>βₒ</code>, an initial inverse temperature, second, <code>k</code>, the current step in the tempering schedule, and third, <code>K</code>, the total number of steps in the tempering schedule.</li></ul><p><strong>Returns</strong></p><ul><li>A <code>NamedTuple</code> with the following keys: <ul><li><code>z_init</code>: The initial latent variable. </li><li><code>ρ_init</code>: The initial momentum variable. </li><li><code>z_final</code>: The final latent variable after <code>K</code> leapfrog steps. </li><li><code>ρ_final</code>: The final momentum variable after <code>K</code> leapfrog steps. </li></ul></li><li>The decoder output at the final latent variable is also returned. Note: This is not in the same named tuple as the other outputs, but as a separate output.</li></ul><p><strong>Description</strong></p><p>The function first samples a random momentum variable <code>γₒ</code> from a standard normal distribution and scales it by the inverse square root of the initial inverse temperature <code>βₒ</code> to obtain the initial momentum variable <code>ρₒ</code>. Then, it performs <code>K</code> leapfrog steps, each followed by a tempering step, to generate a new sample from the latent space.</p><p><strong>Note</strong></p><p>Ensure the input data <code>x</code> and the initial latent variable <code>zₒ</code> match the expected input dimensionality for the HVAE model.</p></div></section><section><div><pre><code class="language-julia hljs">leapfrog_tempering_step(
    x::AbstractArray,
    zₒ::AbstractVecOrMat,
    hvae::HVAE;
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    K::Int=3,
    βₒ::Number=0.3f0,
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=reconstruction_loglikelihood,
        latent_logprior=spherical_logprior,
    ),
    tempering_schedule::Function=quadratic_tempering,
)</code></pre><p>Combines the leapfrog and tempering steps into a single function for the Hamiltonian Variational Autoencoder (HVAE).</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: The data to be processed. If <code>Array</code>, the last dimension must be of size 1.</li><li><code>zₒ::AbstractVecOrMat</code>: The initial latent variable. </li><li><code>hvae::HVAE</code>: An HVAE model that contains the decoder.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}</code>: The step size for the leapfrog steps in the HMC algorithm. This can be a scalar or an array. Default is 0.0001.  </li><li><code>K::Int</code>: The number of leapfrog steps to perform in the Hamiltonian Monte Carlo (HMC) algorithm. Default is 3.</li><li><code>βₒ::Number</code>: The initial inverse temperature for the tempering schedule. Default is 0.3f0.</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: Additional keyword arguments to be passed to the <code>∇potential_energy</code> function. Default is a NamedTuple with <code>reconstruction_loglikelihood</code> and <code>latent_logprior</code>.</li><li><code>tempering_schedule::Function</code>: The function to compute the inverse temperature at each step in the HMC algorithm. Defaults to <code>quadratic_tempering</code>. This function must take three arguments: First, <code>βₒ</code>, an initial inverse temperature, second, <code>k</code>, the current step in the tempering schedule, and third, <code>K</code>, the total number of steps in the tempering schedule.</li></ul><p><strong>Returns</strong></p><ul><li>A <code>NamedTuple</code> with the following keys: <ul><li><code>z_init</code>: The initial latent variable. </li><li><code>ρ_init</code>: The initial momentum variable. </li><li><code>z_final</code>: The final latent variable after <code>K</code> leapfrog steps. </li><li><code>ρ_final</code>: The final momentum variable after <code>K</code> leapfrog steps. </li></ul></li><li>The decoder output at the final latent variable is also returned. Note: This is not in the same named tuple as the other outputs, but as a separate output.</li></ul><p><strong>Description</strong></p><p>The function first samples a random momentum variable <code>γₒ</code> from a standard normal distribution and scales it by the inverse square root of the initial inverse temperature <code>βₒ</code> to obtain the initial momentum variable <code>ρₒ</code>. Then, it performs <code>K</code> leapfrog steps, each followed by a tempering step, to generate a new sample from the latent space.</p><p><strong>Note</strong></p><p>Ensure the input data <code>x</code> and the initial latent variable <code>zₒ</code> match the expected input dimensionality for the HVAE model.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs._log_p̄" href="#AutoEncoderToolkit.HVAEs._log_p̄"><code>AutoEncoderToolkit.HVAEs._log_p̄</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">_log_p̄(
    x::AbstractArray,
    hvae::HVAE{VAE{E,D}},
    hvae_outputs::NamedTuple;
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    logprior::Function=spherical_logprior,
    prefactor::AbstractArray=ones(Float32, 3),
)</code></pre><p>This is an internal function used in <code>hamiltonian_elbo</code> to compute the numerator of the unbiased estimator of the marginal likelihood. The function computes the sum of the log likelihood of the data given the latent variables, the log prior of the latent variables, and the log prior of the momentum variables.</p><pre><code class="nohighlight hljs">    log p̄ = log p(x | zₖ) + log p(zₖ) + log p(ρₖ)</code></pre><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: The input data. If <code>Array</code>, the last dimension must contain each of the data points.</li><li><code>hvae::HVAE{&lt;:VAE{&lt;:AbstractGaussianEncoder,&lt;:AbstractGaussianLogDecoder}}</code>: The Hamiltonian Variational Autoencoder (HVAE) model.</li><li><code>hvae_outputs::NamedTuple</code>: The outputs of the HVAE, including the final latent variables <code>zₖ</code> and the final momentum variables <code>ρₖ</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>reconstruction_loglikelihood::Function</code>: The function to compute the log likelihood of the data given the latent variables. Default is <code>decoder_loglikelihood</code>.</li><li><code>logprior::Function</code>: The function to compute the log prior of the latent variables. Default is <code>spherical_logprior</code>.</li><li><code>prefactor::AbstractArray</code>: A 3-element array to scale the log likelihood, log prior of the latent variables, and log prior of the momentum variables. Default is an array of ones.</li></ul><p><strong>Returns</strong></p><ul><li><code>log_p̄::AbstractVector</code>: The first term of the log of the unbiased estimator of the marginal likelihood for each data point.</li></ul><p><strong>Note</strong></p><p>This is an internal function and should not be called directly. It is used as part of the <code>hamiltonian_elbo</code> function.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs._log_q̄" href="#AutoEncoderToolkit.HVAEs._log_q̄"><code>AutoEncoderToolkit.HVAEs._log_q̄</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">_log_q̄(
    hvae::HVAE,
    hvae_outputs::NamedTuple,
    βₒ::Number;
    logprior::Function=spherical_logprior,
    prefactor::AbstractArray=ones(Float32, 3),
)</code></pre><p>This is an internal function used in <code>hamiltonian_elbo</code> to compute the second term of the unbiased estimator of the marginal likelihood. The function computes the sum of the log posterior of the initial latent variables and the log prior of the initial momentum variables, minus a term that depends on the dimensionality of the latent space and the initial temperature.</p><pre><code class="nohighlight hljs">log q̄ = log q(zₒ | x) + log p(ρₒ | zₒ) - d/2 log(βₒ)</code></pre><p><strong>Arguments</strong></p><ul><li><code>hvae::HVAE</code>: The Hamiltonian Variational Autoencoder (HVAE) model.</li><li><code>hvae_outputs::NamedTuple</code>: The outputs of the HVAE, including the initial latent variables <code>zₒ</code> and the initial momentum variables <code>ρₒ</code>.</li><li><code>βₒ::Number</code>: The initial temperature for the tempering steps.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>logprior::Function</code>: The function to compute the log prior of the momentum variables. Default is <code>spherical_logprior</code>.</li><li><code>prefactor::AbstractArray</code>: A 3-element array to scale the log posterior of the initial latent variables, log prior of the initial momentum variables, and the tempering Jacobian term. Default is an array of ones.</li></ul><p><strong>Returns</strong></p><ul><li><code>log_q̄::Vector</code>: The second term of the log of the unbiased estimator of the marginal likelihood for each data point.</li></ul><p><strong>Note</strong></p><p>This is an internal function and should not be called directly. It is used as part of the <code>hamiltonian_elbo</code> function.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.HVAEs.hamiltonian_elbo" href="#AutoEncoderToolkit.HVAEs.hamiltonian_elbo"><code>AutoEncoderToolkit.HVAEs.hamiltonian_elbo</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">hamiltonian_elbo(
    hvae::HVAE,
    x::AbstractArray;
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    K::Int=3,
    βₒ::Number=0.3f0,
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=decoder_loglikelihood,
        latent_logprior=spherical_logprior,
    ),
    tempering_schedule::Function=quadratic_tempering,
    return_outputs::Bool=false,
    logp_prefactor::AbstractArray=ones(Float32, 3),
    logq_prefactor::AbstractArray=ones(Float32, 3),
)</code></pre><p>Compute the Hamiltonian Monte Carlo (HMC) estimate of the evidence lower bound (ELBO) for a Hamiltonian Variational Autoencoder (HVAE).</p><p>This function takes as input an HVAE and a vector of input data <code>x</code>. It performs <code>K</code> HMC steps with a leapfrog integrator and a tempering schedule to estimate the ELBO. The ELBO is computed as the difference between the <code>log p̄</code> and <code>log q̄</code> as</p><p>elbo = mean(log p̄ - log q̄),</p><p><strong>Arguments</strong></p><ul><li><code>hvae::HVAE</code>: The HVAE used to encode the input data and decode the latent space.</li><li><code>x::AbstractArray</code>: The input data. If <code>Array</code>, the last dimension must contain each of the data points.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}</code>: The step size for the leapfrog integrator (default is 0.01).</li><li><code>K::Int</code>: The number of HMC steps (default is 3).</li><li><code>βₒ::Number</code>: The initial inverse temperature (default is 0.3).</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: Additional keyword arguments to be passed to the <code>∇potential_energy</code> function. Defaults to a NamedTuple with <code>:reconstruction_loglikelihood</code> set to <code>decoder_loglikelihood</code> and <code>:latent_logprior</code> set to <code>spherical_logprior</code>.</li><li><code>tempering_schedule::Function</code>: The tempering schedule function used in the HMC (default is <code>quadratic_tempering</code>).</li><li><code>return_outputs::Bool</code>: Whether to return the outputs of the HVAE. Defaults to <code>false</code>. NOTE: This is necessary to avoid computing the forward pass twice when computing the loss function with regularization.</li><li><code>logp_prefactor::AbstractArray</code>: A 3-element array to scale the log likelihood, log prior of the latent variables, and log prior of the momentum variables. Default is an array of ones.</li><li><code>logq_prefactor::AbstractArray</code>: A 3-element array to scale the log posterior of the initial latent variables, log prior of the initial momentum variables, and the tempering Jacobian term. Default is an array of ones.</li></ul><p><strong>Returns</strong></p><ul><li><code>elbo::Number</code>: The HMC estimate of the ELBO. If <code>return_outputs</code> is <code>true</code>, also returns the outputs of the HVAE.</li></ul></div></section><section><div><pre><code class="language-julia hljs">hamiltonian_elbo(
    hvae::HVAE,
    x_in::AbstractArray,
    x_out::AbstractArray;
    ϵ::Union{&lt;:Number,&lt;:AbstractVector}=Float32(1E-4),
    K::Int=3,
    βₒ::Number=0.3f0,
    ∇U_kwargs::Union{Dict,NamedTuple}=(
        reconstruction_loglikelihood=decoder_loglikelihood,
        latent_logprior=spherical_logprior,
    ),
    tempering_schedule::Function=quadratic_tempering,
    return_outputs::Bool=false,
    logp_prefactor::AbstractArray=ones(Float32, 3),
    logq_prefactor::AbstractArray=ones(Float32, 3),
)</code></pre><p>Compute the Hamiltonian Monte Carlo (HMC) estimate of the evidence lower bound (ELBO) for a Hamiltonian Variational Autoencoder (HVAE).</p><p>This function takes as input an HVAE and a vector of input data <code>x</code>. It performs <code>K</code> HMC steps with a leapfrog integrator and a tempering schedule to estimate the ELBO. The ELBO is computed as the difference between the <code>log p̄</code> and <code>log q̄</code> as</p><p>elbo = mean(log p̄ - log q̄),</p><p><strong>Arguments</strong></p><ul><li><code>hvae::HVAE</code>: The HVAE used to encode the input data and decode the latent space.</li><li><code>x_in::AbstractArray</code>: The input data. If <code>Array</code>, the last dimension must contain each of the data points.</li><li><code>x_out::AbstractArray</code>: The data against which the reconstruction is compared. If <code>Array</code>, the last dimension must contain each of the data points.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>ϵ::Union{&lt;:Number,&lt;:AbstractVector}</code>: The step size for the leapfrog integrator (default is 0.01).</li><li><code>K::Int</code>: The number of HMC steps (default is 3).</li><li><code>βₒ::Number</code>: The initial inverse temperature (default is 0.3).</li><li><code>∇U_kwargs::Union{Dict,NamedTuple}</code>: Additional keyword arguments to be passed to the <code>∇potential_energy</code> function. Defaults to a NamedTuple with <code>:reconstruction_loglikelihood</code> set to <code>decoder_loglikelihood</code> and <code>:latent_logprior</code> set to <code>spherical_logprior</code>.</li><li><code>tempering_schedule::Function</code>: The tempering schedule function used in the HMC (default is <code>quadratic_tempering</code>).</li><li><code>return_outputs::Bool</code>: Whether to return the outputs of the HVAE. Defaults to <code>false</code>. NOTE: This is necessary to avoid computing the forward pass twice when computing the loss function with regularization.</li><li><code>logp_prefactor::AbstractArray</code>: A 3-element array to scale the log likelihood, log prior of the latent variables, and log prior of the momentum variables. Default is an array of ones.</li><li><code>logq_prefactor::AbstractArray</code>: A 3-element array to scale the log posterior of the initial latent variables, log prior of the initial momentum variables, and the tempering Jacobian term. Default is an array of ones.</li></ul><p><strong>Returns</strong></p><ul><li><code>elbo::Number</code>: The HMC estimate of the ELBO. If <code>return_outputs</code> is <code>true</code>, also returns the outputs of the HVAE.</li></ul></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../infomaxvae/">« InfoMax-VAE</a><a class="docs-footer-nextpage" href="../rhvae/">RHVAE »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Friday 10 May 2024 19:35">Friday 10 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
