<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Differential Geometry · AutoEncoderToolkit</title><meta name="title" content="Differential Geometry · AutoEncoderToolkit"/><meta property="og:title" content="Differential Geometry · AutoEncoderToolkit"/><meta property="twitter:title" content="Differential Geometry · AutoEncoderToolkit"/><meta name="description" content="Documentation for AutoEncoderToolkit."/><meta property="og:description" content="Documentation for AutoEncoderToolkit."/><meta property="twitter:description" content="Documentation for AutoEncoderToolkit."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.svg" alt="AutoEncoderToolkit logo"/><img class="docs-dark-only" src="../assets/logo-dark.svg" alt="AutoEncoderToolkit logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AutoEncoderToolkit</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><a class="tocitem" href="../encoders/">Encoders &amp; Decoders</a></li><li><a class="tocitem" href="../layers/">Custom Layers</a></li><li><a class="tocitem" href="../ae/">Deterministic Autoencoders</a></li><li><a class="tocitem" href="../vae/">VAE / β-VAE</a></li><li><a class="tocitem" href="../mmdvae/">MMD-VAE (InfoVAE)</a></li><li><a class="tocitem" href="../infomaxvae/">InfoMax-VAE</a></li><li><a class="tocitem" href="../hvae/">HVAE</a></li><li><a class="tocitem" href="../rhvae/">RHVAE</a></li><li class="is-active"><a class="tocitem" href>Differential Geometry</a><ul class="internal"><li><a class="tocitem" href="#A-word-on-Riemannian-geometry"><span>A word on Riemannian geometry</span></a></li><li><a class="tocitem" href="#neuralgeodesic"><span>Neural Geodesic Networks</span></a></li><li><a class="tocitem" href="#diffgeoref"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../utils/">Utilities</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Differential Geometry</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Differential Geometry</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Differential-Geometry-of-Generative-Models"><a class="docs-heading-anchor" href="#Differential-Geometry-of-Generative-Models">Differential Geometry of Generative Models</a><a id="Differential-Geometry-of-Generative-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-Geometry-of-Generative-Models" title="Permalink"></a></h1><p>A lot of recent research in the field of generative models has focused on the geometry of the learned latent space (see the <a href="#diffgeoref">references</a> at the end of this section for examples). The non-linear nature of neural networks makes it relevant to consider the non-Euclidean geometry of the latent space when trying to gain insights into the structure of the learned space. In other words, given that neural networks involve a series of non-linear transformations of the input data, we cannot expect the latent space to be Euclidean, and thus, we need to account for curvature and other non-Euclidean properties. For this, we can borrow concepts and tools from Riemannian geometry, now applied to the latent space of generative models.</p><p><code>AutoEncoderToolkit.jl</code> aims to provide the set of necessary tools to study the geometry of the latent space in the context of variational autoencoders generative models.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This is very much work in progress. As always, contributions are welcome!</p></div></div><h2 id="A-word-on-Riemannian-geometry"><a class="docs-heading-anchor" href="#A-word-on-Riemannian-geometry">A word on Riemannian geometry</a><a id="A-word-on-Riemannian-geometry-1"></a><a class="docs-heading-anchor-permalink" href="#A-word-on-Riemannian-geometry" title="Permalink"></a></h2><p>In what follows we will give a very short primer on some relevant concepts in differential geometry. This includes some basic definitions and concepts along with what we consider intuitive explanations of the concepts. We trade rigor for accessibility, so if you are looking for a more formal treatment, this is not the place.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>These notes are partially based on the 2022 paper by Chadebec et al. [2].</p></div></div><p>A <span>$d$</span>-dimensional manifold <span>$\mathcal{M}$</span> is a manifold that is locally homeomorphic to a <span>$d$</span>-dimensional Euclidean space. This means that the manifold–some surface or high-dimensional shape–when observed from really close, can be stretched or bent without tearing or gluing it to make it resemble regular Euclidean space. </p><p>If the manifold is differentiable, it possesses a tangent space <span>$T_z$</span> at any point <span>$z \in \mathcal{M}$</span> composed of the tangent vectors of the curves passing by <span>$z$</span>. </p><p><img src="../figs/diffgeo01.png" alt/></p><p>If the manifold <span>$\mathcal{M}$</span> is equipped with a smooth inner product, </p><p class="math-container">\[g: z \rightarrow \langle \cdot \mid \cdot \rangle_z,
\tag{1}\]</p><p>defined on the tangent space <span>$T_z$</span> for any <span>$z \in \mathcal{M}$</span>, then <span>$\mathcal{M}$</span> is a Riemannian manifold and <span>$g$</span> is the associated Riemannian metric. With this, a local representation of <span>$g$</span> at any point <span>$z$</span> is given by the positive definite matrix <span>$\mathbf{G}(z)$</span>.</p><p>A chart (fancy name for a coordinate system) <span>$(U, \phi)$</span> provides a homeomorphic mapping between an open set <span>$U$</span> of the manifold and an open set <span>$V$</span> of Euclidean space. This means that there is a way to bend and stretch any segment of the manifold to make it look like a segment of Euclidean space. Therefore, given a point <span>$z \in U$</span>, a chart–its coordinate–<span>$\phi: (z_1, z_2, \ldots, z_d)$</span> induces a basis <span>$\{\partial_{z_1}, \partial_{z_2}, \ldots, \partial_{z_d}\}$</span> on the tangent space <span>$T_z \mathcal{M}$</span>. In other words, the partial derivatives of the manifold with respect to the dimensions form a basis (think of <span>$\hat{i}, \hat{j}, \hat{k}$</span> in 3D space) for the tangent space at that point. Hence, the metric–a &quot;position-dependent scale-bar&quot;–of a Riemannian manifold can be locally represented at <span>$\phi$</span> as a positive definite matrix <span>$\mathbf{G}(z)$</span> with components <span>$g_{ij}(z)$</span> of the form</p><p class="math-container">\[g_{ij}(z) = \langle \partial_{z_i} \mid \partial_{z_j} \rangle_z.
\tag{2}\]</p><p>This implies that for every pair of vectors <span>$v, w \in T_z \mathcal{M}$</span> and a point <span>$z \in \mathcal{M}$</span>, the inner product <span>$\langle v \mid w \rangle_z$</span> is given by</p><p class="math-container">\[\langle v \mid w \rangle_z = v^T \mathbf{G}(z) w.
\tag{3}\]</p><p>If <span>$\mathcal{M}$</span> is connected–a continuous shape with no breaks–a Riemannian distance between two points <span>$z_1, z_2 \in \mathcal{M}$</span> can be defined as</p><p class="math-container">\[\text{dist}(z_1, z_2) = \min_{\gamma} \int_0^1 dt
\sqrt{\langle \dot{\gamma}(t) \mid \dot{\gamma}(t) \rangle_{\gamma(t)}},
\tag{4}\]</p><p>where <span>$\gamma$</span> is a 1D curve traveling from <span>$z_1$</span> to <span>$z_2$</span>, i.e., <span>$\gamma(0) = z_1$</span> and <span>$\gamma(1) = z_2$</span>. Another way to state this is that the length of a curve on the manifold <span>$\gamma$</span> is given by</p><p class="math-container">\[L(\gamma) = \int_0^1 dt 
\sqrt{\langle \dot{\gamma}(t) \mid \dot{\gamma}(t) \rangle_{\gamma(t)}}.
\tag{5}\]</p><p>If <span>$L$</span> minimizes the distance between the initial and final points, then <span>$\gamma$</span> is a <strong>geodesic curve</strong>.</p><p>The concept of geodesic is so important the study of the Riemannian manifold learned by generative models that let&#39;s try to give another intuitive explanation. Let us consider a curve <span>$\gamma$</span> such that</p><p class="math-container">\[\gamma: [0, 1] \rightarrow \mathbb{R}^d,
\tag{6}\]</p><p>In words, <span>$\gamma$</span> is a function that, without loss of generality, maps a number between zero and one to the dimensionality of the latent space (the dimensionality of our manifold). Let us define <span>$f$</span> to be a continuous function  that embeds any point along the curve <span>$\gamma$</span> into the data space, i.e.,</p><p class="math-container">\[f : \gamma(t) \rightarrow x \in \mathbb{R}^n.
\tag{7}\]</p><p>where <span>$n$</span> is the dimensionality of the data space. </p><p><img src="../figs/diffgeo02.png" alt/></p><p>The length of this curve in the data space is given by</p><p class="math-container">\[L(\gamma) = \int_0^1 dt
\left\| \frac{d f}{dt} \right\|_2.
\tag{8}\]</p><p>After some manipulation, we can show that the length of the curve in the data space is given by</p><p class="math-container">\[L(\gamma) = \int_0^1 dt
\sqrt{
    \dot{\gamma}(t)^T \mathbf{G}(\gamma(t)) \dot{\gamma}(t)
},
\tag{9}\]</p><p>where <span>$\dot{\gamma}(t)$</span> is the derivative of <span>$\gamma$</span> with respect to <span>$t$</span>, and <span>$T$</span> denotes the transpose of a vector. For a Euclidean space, the length of the curve would take the same functional form, except that the metric tensor would be given by the identity matrix. This is why the metric tensor can be  thought of as a position-dependent scale-bar.</p><h2 id="neuralgeodesic"><a class="docs-heading-anchor" href="#neuralgeodesic">Neural Geodesic Networks</a><a id="neuralgeodesic-1"></a><a class="docs-heading-anchor-permalink" href="#neuralgeodesic" title="Permalink"></a></h2><p>Computing a geodesic on a Riemannian manifold is a non-trivial task, especially when the manifold is parametrized by a neural network. Thus, knowing the  function <span>$\gamma$</span> that minimizes the distance between two points <span>$z_1$</span> and <span>$z_2$</span> is not straightforward. However, as first suggested by Chen et al. [1], we can repurpose the expressivity of neural networks to approximate almost any function to approximate the geodesic curve. This is the idea behind the Neural Geodesic module in <code>AutoEncoderToolkit.jl</code>.</p><p>Briefly, to approximate the geodesic curve between two points <span>$z_1$</span> and <span>$z_2$</span> in latent space, we define a neural network <span>$g_\omega$</span> such that</p><p class="math-container">\[g_\omega: \mathbb{R} \rightarrow \mathbb{R}^d,
\tag{10}\]</p><p>i.e., the neural network takes a number between zero and one and maps it to the dimensionality of the latent space. The intention is to have <span>$g_\omega \approx \gamma$</span>, where <span>$\omega$</span> are the parameters of the neural network we are free to optimize.</p><p>We approximate the integral defining the length of the curve in the latent space with <span>$n$</span> equidistantly sampled points <span>$t_i$</span> between zero and one. The length of the curve is then approximated by</p><p class="math-container">\[L(g_\gamma(t)) \approx \frac{1}{n} \sum_{i=1}^n 
\sqrt{
    \dot{g}_\omega(t_i)^T \mathbf{G}(g_\omega(t_i)) \dot{g}_\omega(t_i)
},\]</p><p>By setting the loss function to be this approximation of the length of the curve, we can train the neural network to approximate the geodesic curve.</p><p><code>AutoEncoderToolkit.jl</code> provides the <code>NeuralGeodesic</code> struct to implement this idea. The struct takes three inputs:</p><ul><li>The multi-layer perceptron (MLP) that approximates the geodesic curve.</li><li>The initial point in latent space.</li><li>The final point in latent space.</li></ul><h3 id="NeuralGeodesic-struct"><a class="docs-heading-anchor" href="#NeuralGeodesic-struct"><code>NeuralGeodesic</code> struct</a><a id="NeuralGeodesic-struct-1"></a><a class="docs-heading-anchor-permalink" href="#NeuralGeodesic-struct" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.NeuralGeodesic" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.NeuralGeodesic"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.NeuralGeodesic</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NeuralGeodesic</code></pre><p>Type to define a neural network that approximates a geodesic curve on a Riemanian manifold. If a curve γ̲(t) represents a geodesic curve on a manifold, i.e.,</p><pre><code class="nohighlight hljs">L(γ̲) = min_γ ∫ dt √(⟨γ̲̇(t), M̲̲ γ̲̇(t)⟩),</code></pre><p>where M̲̲ is the Riemmanian metric, then this type defines a neural network g_ω(t) such that</p><pre><code class="nohighlight hljs">γ̲(t) ≈ g_ω(t).</code></pre><p>This neural network must have a single input (1D). The dimensionality of the output must match the dimensionality of the manifold.</p><p><strong>Fields</strong></p><ul><li><code>mlp::Flux.Chain</code>: Neural network that approximates the geodesic curve. The dimensionality of the input must be one.</li><li><code>z_init::AbstractVector</code>: Initial position of the geodesic curve on the latent space.</li><li><code>z_end::AbstractVector</code>: Final position of the geodesic curve on the latent space.</li></ul><p><strong>Citation</strong></p><blockquote><p>Chen, N. et al. Metrics for Deep Generative Models. in Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics 1540–1550 (PMLR, 2018).</p></blockquote></div></section></article><h3 id="NeuralGeodesic-forward-pass"><a class="docs-heading-anchor" href="#NeuralGeodesic-forward-pass"><code>NeuralGeodesic</code> forward pass</a><a id="NeuralGeodesic-forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#NeuralGeodesic-forward-pass" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.NeuralGeodesic-Tuple{AbstractVector}" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.NeuralGeodesic-Tuple{AbstractVector}"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.NeuralGeodesic</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (g::NeuralGeodesic)(t::AbstractArray)</code></pre><p>Computes the output of the NeuralGeodesic at each given time in <code>t</code> by scaling and shifting the output of the neural network.</p><p><strong>Arguments</strong></p><ul><li><code>t::AbstractArray</code>: An array of times at which the output of the NeuralGeodesic is to be computed. This must be within the interval [0, 1].</li></ul><p><strong>Returns</strong></p><ul><li><code>output::Array</code>: The computed output of the NeuralGeodesic at each time in <code>t</code>.</li></ul><p><strong>Description</strong></p><p>The function computes the output of the NeuralGeodesic at each given time in <code>t</code>. The steps are:</p><ol><li>Compute the output of the neural network at each time in <code>t</code>.</li><li>Compute the output of the neural network at time 0 and 1.</li><li>Compute scale and shift parameters based on the initial and end points of the geodesic and the neural network outputs at times 0 and 1.</li><li>Scale and shift the output of the neural network at each time in <code>t</code> according to these parameters. The result is the output of the NeuralGeodesic at each time in <code>t</code>.</li></ol><p>Scale and shift parameters are defined as:</p><ul><li>scale = (z<em>init - z</em>end) / (ẑ<em>init - ẑ</em>end)</li><li>shift = (z<em>init * ẑ</em>end - z<em>end * ẑ</em>init) / (ẑ<em>init - ẑ</em>end)</li></ul><p>where z<em>init and z</em>end are the initial and end points of the geodesic, and ẑ<em>init and ẑ</em>end are the outputs of the neural network at times 0 and 1, respectively.</p><p><strong>Note</strong></p><p>Ensure that each <code>t</code> in the array is within the interval [0, 1].</p></div></section></article><h3 id="NeuralGeodesic-loss-function"><a class="docs-heading-anchor" href="#NeuralGeodesic-loss-function"><code>NeuralGeodesic</code> loss function</a><a id="NeuralGeodesic-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#NeuralGeodesic-loss-function" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.loss" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.loss"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">loss(
    curve::NeuralGeodesic,
    rhvae::RHVAE,
    t::AbstractVector;
    curve_velocity::Function=curve_velocity_TaylorDiff,
    curve_integral::Function=curve_length,
)</code></pre><p>Function to compute the loss for a given curve on a Riemmanian manifold. The loss is defined as the integral over the curve, computed using the provided <code>curve_integral</code> function (either length or energy).</p><p><strong>Arguments</strong></p><ul><li><code>curve::NeuralGeodesic</code>: The curve on the Riemmanian manifold.</li><li><code>rhvae::RHVAE</code>: The Riemmanian Hamiltonian Variational AutoEncoder used to compute the Riemmanian metric tensor.</li><li><code>t::AbstractVector</code>: Vector of time points at which the curve is sampled.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>curve_velocity::Function=curve_velocity_TaylorDiff</code>: Function to compute the velocity of the curve. Default is <code>curve_velocity_TaylorDiff</code>. Also accepts <code>curve_velocity_finitediff</code>.</li><li><code>curve_integral::Function=curve_length</code>: Function to compute the integral over the curve. Default is <code>curve_energy</code>. Also accepts <code>curve_length</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>Loss::Number</code>: The computed loss for the given curve.</li></ul><p><strong>Notes</strong></p><p>This function first computes the geodesic curve using the provided <code>curve</code> function. It then computes the Riemmanian metric tensor using the <code>metric_tensor</code> function from the <code>RHVAE</code> module with the computed curve and the provided <code>rhvae</code>. The velocity of the curve is then computed using the provided <code>curve_velocity</code> function. Finally, the integral over the curve is computed using the provided <code>curve_integral</code> function and returned as the loss.</p></div></section></article><h3 id="NeuralGeodesic-training"><a class="docs-heading-anchor" href="#NeuralGeodesic-training"><code>NeuralGeodesic</code> training</a><a id="NeuralGeodesic-training-1"></a><a class="docs-heading-anchor-permalink" href="#NeuralGeodesic-training" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.train!" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.train!"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train!(
    curve::NeuralGeodesic,
    rhvae::RHVAE,
    t::AbstractVector,
    opt::NamedTuple;
    loss::Function=loss,
    loss_kwargs::Dict=Dict(),
    verbose::Bool=false,
    loss_return::Bool=false,
)</code></pre><p>Function to train a NeuralGeodesic model using a Riemmanian Hamiltonian Variational AutoEncoder (RHVAE). The training process involves computing the gradient of the loss function and updating the model parameters accordingly.</p><p><strong>Arguments</strong></p><ul><li><code>curve::NeuralGeodesic</code>: The curve on the Riemmanian manifold.</li><li><code>rhvae::RHVAE</code>: The Riemmanian Hamiltonian Variational AutoEncoder used to compute the Riemmanian metric tensor.</li><li><code>t::AbstractVector</code>: Vector of time points at which the curve is sampled. These must be equally spaced.</li><li><code>opt::NamedTuple</code>: The optimization parameters.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>loss_function::Function=loss</code>: The loss function to be minimized during training. Default is <code>loss</code>.</li><li><code>loss_kwargs::Dict=Dict()</code>: Additional keyword arguments to be passed to the loss function.</li><li><code>verbose::Bool=false</code>: If <code>true</code>, the loss value is printed at each iteration.</li><li><code>loss_return::Bool=false</code>: If <code>true</code>, the function returns the loss value.</li></ul><p><strong>Returns</strong></p><ul><li><code>Loss::Number</code>: The computed loss for the given curve. This is only returned if <code>loss_return</code> is <code>true</code>.</li></ul><p><strong>Notes</strong></p><p>This function first computes the gradient of the loss function with respect to the model parameters. It then updates the model parameters using the computed gradient and the provided optimization parameters. If <code>verbose</code> is <code>true</code>, the loss value is printed at each iteration. If <code>loss_return</code> is <code>true</code>, the function returns the loss value.</p></div></section></article><h3 id="Other-functions-for-NeuralGeodesic"><a class="docs-heading-anchor" href="#Other-functions-for-NeuralGeodesic">Other functions for <code>NeuralGeodesic</code></a><a id="Other-functions-for-NeuralGeodesic-1"></a><a class="docs-heading-anchor-permalink" href="#Other-functions-for-NeuralGeodesic" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_velocity_TaylorDiff" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_velocity_TaylorDiff"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_velocity_TaylorDiff</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">curve_velocity_TaylorDiff(
    curve::NeuralGeodesic,
    t
)</code></pre><p>Compute the velocity of a neural geodesic curve at a given time using Taylor differentiation.</p><p>This function takes a <code>NeuralGeodesic</code> instance and a time <code>t</code>, and computes the velocity of the curve at that time using Taylor differentiation. The computation is performed for each dimension of the latent space.</p><p><strong>Arguments</strong></p><ul><li><code>curve::NeuralGeodesic</code>: The neural geodesic curve.</li><li><code>t</code>: The time at which to compute the velocity.</li></ul><p><strong>Returns</strong></p><p>A vector representing the velocity of the curve at time <code>t</code>.</p><p><strong>Notes</strong></p><p>This function uses the <code>TaylorDiff</code> package to compute derivatives. Please note that <code>TaylorDiff</code> has limited support for certain activation functions. If you encounter an error while using this function, it may be due to the activation function used in your <code>NeuralGeodesic</code> instance.</p></div></section><section><div><pre><code class="language-julia hljs">curve_velocity_TaylorDiff(
    curve::NeuralGeodesic,
    t::AbstractVector
)</code></pre><p>Compute the velocity of a neural geodesic curve at each time in a vector of times using Taylor differentiation.</p><p>This function takes a <code>NeuralGeodesic</code> instance and a vector of times <code>t</code>, and computes the velocity of the curve at each time using Taylor differentiation. The computation is performed for each dimension of the latent space and each time in <code>t</code>.</p><p><strong>Arguments</strong></p><ul><li><code>curve::NeuralGeodesic</code>: The neural geodesic curve.</li><li><code>t::AbstractVector</code>: The vector of times at which to compute the velocity.</li></ul><p><strong>Returns</strong></p><p>A matrix where each column represents the velocity of the curve at a time in <code>t</code>.</p><p><strong>Notes</strong></p><p>This function uses the <code>TaylorDiff</code> package to compute derivatives. Please note that <code>TaylorDiff</code> has limited support for certain activation functions. If you encounter an error while using this function, it may be due to the activation function used in your <code>NeuralGeodesic</code> instance.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_velocity_finitediff" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_velocity_finitediff"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_velocity_finitediff</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">curve_velocity_finitediff(
    curve::NeuralGeodesic,
    t::AbstractVector;
    fdtype::Symbol=:central,
)</code></pre><p>Compute the velocity of a neural geodesic curve at each time in a vector of times using finite difference methods.</p><p>This function takes a <code>NeuralGeodesic</code> instance, a vector of times <code>t</code>, and an optional finite difference type <code>fdtype</code> (which can be either <code>:forward</code> or <code>:central</code>), and computes the velocity of the curve at each time using the specified finite difference method. The computation is performed for each dimension of the latent space and each time in <code>t</code>.</p><p><strong>Arguments</strong></p><ul><li><code>curve::NeuralGeodesic</code>: The neural geodesic curve.</li><li><code>t::AbstractVector</code>: The vector of times at which to compute the velocity.</li><li><code>fdtype::Symbol=:central</code>: The type of finite difference method to use. Can be either <code>:forward</code> or <code>:central</code>. Default is <code>:central</code>.</li></ul><p><strong>Returns</strong></p><p>A matrix where each column represents the velocity of the curve at a time in <code>t</code>.</p><p><strong>Notes</strong></p><p>This function uses finite difference methods to compute derivatives. Please note that the accuracy of the computed velocities depends on the chosen finite difference method and the step size used, which is determined by the machine epsilon of the type of <code>t</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_length" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_length"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_length</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">curve_length(
    riemannian_metric::AbstractArray,
    curve_velocity::AbstractArray,
    t::AbstractVector;
)</code></pre><p>Function to compute the (discretized) integral defining the length of a curve γ̲ on a Riemmanina manifold. The length is defined as</p><pre><code class="nohighlight hljs">L(γ̲) = ∫ dt √(⟨γ̲̇(t), G̲̲ γ̲̇(t)⟩),</code></pre><p>where γ̲̇(t) defines the velocity of the parametric curve, and G̲̲ is the Riemmanian metric tensor. For this function, we approximate the integral as</p><pre><code class="nohighlight hljs">L(γ̲) ≈ ∑ᵢ Δt √(⟨γ̲̇(tᵢ)ᵀ G̲̲ (γ̲(tᵢ+1)) γ̲̇(tᵢ))⟩),</code></pre><p>where Δt is the time step between points. Note that this Δt is assumed to be constant, thus, the time points <code>t</code> must be equally spaced.</p><p><strong>Arguments</strong></p><ul><li><code>riemannian_metric::AbstractArray</code>: <code>d×d×N</code> tensor where <code>d</code> is the dimension of the manifold on which the curve lies and <code>N</code> is the number of sampled time points along the curve. Each slice of the array represents the Riemmanian metric tensor for the curve at the corresponding time point.</li><li><code>curve_velocity::AbstractArray</code>: <code>d×N</code> Matrix where <code>d</code> is the dimension of the manifold on which the curve lies and <code>N</code> is the number of sampled time points along the curve. Each column represents the velocity of the curve at the corresponding time point.</li><li><code>t::AbstractVector</code>: Vector of time points at which the curve is sampled.</li></ul><p><strong>Returns</strong></p><ul><li><code>Length::Number</code>: Approximation of the Length for the path on the manifold.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_energy" href="#AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_energy"><code>AutoEncoderToolkit.diffgeo.NeuralGeodesics.curve_energy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">curve_energy(
    riemannian_metric::AbstractArray,
    curve_velocity::AbstractArray,
    t::AbstractVector;
)</code></pre><p>Function to compute the (discretized) integral defining the energy of a curve γ̲ on a Riemmanina manifold. The energy is defined as</p><pre><code class="nohighlight hljs">    E(γ̲) = ∫ dt ⟨γ̲̇(t), G̲̲ γ̲̇(t)⟩,</code></pre><p>where γ̲̇(t) defines the velocity of the parametric curve, and G̲̲ is the Riemmanian metric tensor. For this function, we approximate the integral as</p><pre><code class="nohighlight hljs">    E(γ̲) ≈ ∑ᵢ Δt ⟨γ̲̇(tᵢ)ᵀ G̲̲ (γ̲(tᵢ+1) γ̲̇(tᵢ))⟩,</code></pre><p>where Δt is the time step between points. Note that this Δt is assumed to be constant, thus, the time points <code>t</code> must be equally spaced.</p><p><strong>Arguments</strong></p><ul><li><code>riemannian_metric::AbstractArray</code>: <code>d×d×N</code> tensor where <code>d</code> is the dimension of the manifold on which the curve lies and <code>N</code> is the number of sampled time points along the curve. Each slice of the array represents the Riemmanian metric tensor for the curve at the corresponding time point.</li><li><code>curve_velocity::AbstractArray</code>: <code>d×N</code> Matrix where <code>d</code> is the dimension of the manifold on which the curve lies and <code>N</code> is the number of sampled time points along the curve. Each column represents the velocity of the curve at the corresponding time point.</li><li><code>t::AbstractVector</code>: Vector of time points at which the curve is sampled.</li></ul><p><strong>Returns</strong></p><ul><li><code>Energy::Number</code>: Approximation of the Energy for the path on the manifold.</li></ul></div></section></article><h2 id="diffgeoref"><a class="docs-heading-anchor" href="#diffgeoref">References</a><a id="diffgeoref-1"></a><a class="docs-heading-anchor-permalink" href="#diffgeoref" title="Permalink"></a></h2><ol><li>Chen, N. et al. Metrics for Deep Generative Models. in Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics 1540–1550 (PMLR, 2018).</li><li>Chadebec, C. &amp; Allassonnière, S. A Geometric Perspective on Variational Autoencoders. Preprint at http://arxiv.org/abs/2209.07370 (2022).</li><li>Chadebec, C., Mantoux, C. &amp; Allassonnière, S. Geometry-Aware Hamiltonian Variational Auto-Encoder. Preprint at http://arxiv.org/abs/2010.11518 (2020).</li><li>Arvanitidis, G., Hauberg, S., Hennig, P. &amp; Schober, M. Fast and Robust Shortest Paths on Manifolds Learned from Data. in Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics 1506–1515 (PMLR, 2019).</li><li>Arvanitidis, G., Hauberg, S. &amp; Schölkopf, B. Geometrically Enriched Latent Spaces. Preprint at https://doi.org/10.48550/arXiv.2008.00565 (2020).</li><li>Arvanitidis, G., González-Duque, M., Pouplin, A., Kalatzis, D. &amp; Hauberg, S. Pulling back information geometry. Preprint at http://arxiv.org/abs/2106.05367 (2022).</li><li>Fröhlich, C., Gessner, A., Hennig, P., Schölkopf, B. &amp; Arvanitidis, G. Bayesian Quadrature on Riemannian Data Manifolds.</li><li>Kalatzis, D., Eklund, D., Arvanitidis, G. &amp; Hauberg, S. Variational Autoencoders with Riemannian Brownian Motion Priors. Preprint at http://arxiv.org/abs/2002.05227 (2020).</li><li>Arvanitidis, G., Hansen, L. K. &amp; Hauberg, S. Latent Space Oddity: on the Curvature of Deep Generative Models. Preprint at http://arxiv.org/abs/1710.11379 (2021).</li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rhvae/">« RHVAE</a><a class="docs-footer-nextpage" href="../utils/">Utilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Friday 21 June 2024 15:57">Friday 21 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
