<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>VAE / β-VAE · AutoEncoderToolkit</title><meta name="title" content="VAE / β-VAE · AutoEncoderToolkit"/><meta property="og:title" content="VAE / β-VAE · AutoEncoderToolkit"/><meta property="twitter:title" content="VAE / β-VAE · AutoEncoderToolkit"/><meta name="description" content="Documentation for AutoEncoderToolkit."/><meta property="og:description" content="Documentation for AutoEncoderToolkit."/><meta property="twitter:description" content="Documentation for AutoEncoderToolkit."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.svg" alt="AutoEncoderToolkit logo"/><img class="docs-dark-only" src="../assets/logo-dark.svg" alt="AutoEncoderToolkit logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AutoEncoderToolkit</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><a class="tocitem" href="../encoders/">Encoders &amp; Decoders</a></li><li><a class="tocitem" href="../layers/">Custom Layers</a></li><li><a class="tocitem" href="../ae/">Deterministic Autoencoders</a></li><li class="is-active"><a class="tocitem" href>VAE / β-VAE</a><ul class="internal"><li><a class="tocitem" href="#References"><span>References</span></a></li><li><a class="tocitem" href="#VAEstruct"><span><code>VAE</code> struct</span></a></li><li><a class="tocitem" href="#Forward-pass"><span>Forward pass</span></a></li><li><a class="tocitem" href="#Loss-function"><span>Loss function</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li></ul></li><li><a class="tocitem" href="../mmdvae/">MMD-VAE (InfoVAE)</a></li><li><a class="tocitem" href="../infomaxvae/">InfoMax-VAE</a></li><li><a class="tocitem" href="../hvae/">HVAE</a></li><li><a class="tocitem" href="../rhvae/">RHVAE</a></li><li><a class="tocitem" href="../diffgeo/">Differential Geometry</a></li><li><a class="tocitem" href="../utils/">Utilities</a></li><li><a class="tocitem" href="../guidelines/">Community Guidelines</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>VAE / β-VAE</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>VAE / β-VAE</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="VAEsmodule"><a class="docs-heading-anchor" href="#VAEsmodule">β-Variational Autoencoder</a><a id="VAEsmodule-1"></a><a class="docs-heading-anchor-permalink" href="#VAEsmodule" title="Permalink"></a></h1><p>Variational Autoencoders, first introduced by Kingma and Welling in 2014, are a type of generative model that learns to encode high-dimensional data into a low-dimensional latent space. The main idea behind VAEs is to learn a probabilistic mapping (via variational inference) from the input data to the latent space, which allows for the generation of new data points by sampling from the latent space.</p><p>Their counterpart, the β-VAE, introduced by Higgins et al. in 2017, is a variant of the original VAE that includes a hyperparameter <code>β</code> that controls the relative importance of the reconstruction loss and the KL divergence term in the loss function. By adjusting <code>β</code>, the user can control the trade-off between the reconstruction quality and the disentanglement of the latent space.</p><p>In terms of implementation, the <code>VAE</code> struct in <code>AutoEncoderToolkit.jl</code> is a simple feedforward network composed of variational <a href="../encoders/#Encoders">encoder</a> and <a href="../encoders/#Decoders">decoder</a> parts. This means that the encoder has a log-posterior function and a KL divergence function associated with it, while the decoder has a log-likehood function associated with it.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><h3 id="VAE"><a class="docs-heading-anchor" href="#VAE">VAE</a><a id="VAE-1"></a><a class="docs-heading-anchor-permalink" href="#VAE" title="Permalink"></a></h3><blockquote><p>Kingma, D. P. &amp; Welling, M. Auto-Encoding Variational Bayes. Preprint at http://arxiv.org/abs/1312.6114 (2014).</p></blockquote><h3 id="β-VAE"><a class="docs-heading-anchor" href="#β-VAE">β-VAE</a><a id="β-VAE-1"></a><a class="docs-heading-anchor-permalink" href="#β-VAE" title="Permalink"></a></h3><blockquote><p>Higgins, I. et al. β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK. (2017).</p></blockquote><h2 id="VAEstruct"><a class="docs-heading-anchor" href="#VAEstruct"><code>VAE</code> struct</a><a id="VAEstruct-1"></a><a class="docs-heading-anchor-permalink" href="#VAEstruct" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.VAEs.VAE" href="#AutoEncoderToolkit.VAEs.VAE"><code>AutoEncoderToolkit.VAEs.VAE</code></a> — <span class="docstring-category">Type</span></header><section><div><p><code>struct VAE{E&lt;:AbstractVariationalEncoder, D&lt;:AbstractVariationalDecoder}</code></p><p>Variational autoencoder (VAE) model defined for <code>Flux.jl</code></p><p><strong>Fields</strong></p><ul><li><code>encoder::E</code>: Neural network that encodes the input into the latent space. <code>E</code> is a subtype of <code>AbstractVariationalEncoder</code>.</li><li><code>decoder::D</code>: Neural network that decodes the latent representation back to the original input space. <code>D</code> is a subtype of <code>AbstractVariationalDecoder</code>.</li></ul><p>A VAE consists of an encoder and decoder network with a bottleneck latent space in between. The encoder compresses the input into a low-dimensional probabilistic representation q(z|x). The decoder tries to reconstruct the original input from a sampled point in the latent space p(x|z). </p></div></section></article><h2 id="Forward-pass"><a class="docs-heading-anchor" href="#Forward-pass">Forward pass</a><a id="Forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-pass" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.VAEs.VAE-Tuple{AbstractArray}" href="#AutoEncoderToolkit.VAEs.VAE-Tuple{AbstractArray}"><code>AutoEncoderToolkit.VAEs.VAE</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    (vae::VAE)(x::AbstractArray; latent::Bool=false)</code></pre><p>Perform the forward pass of a Variational Autoencoder (VAE).</p><p>This function takes as input a VAE and a vector or matrix of input data <code>x</code>. It first runs the input through the encoder to obtain the mean and log standard deviation of the latent variables. It then uses the reparameterization trick to sample from the latent distribution. Finally, it runs the latent sample through the decoder to obtain the output.</p><p><strong>Arguments</strong></p><ul><li><code>vae::VAE</code>: The VAE used to encode the input data and decode the latent space.</li><li><code>x::AbstractArray</code>: The input data. If array, the last dimension contains each of the samples in a batch.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>latent::Bool</code>: Whether to return the latent variables along with the decoder output. If <code>true</code>, the function returns a tuple containing the encoder outputs, the latent sample, and the decoder outputs. If <code>false</code>, the function only returns the decoder outputs. Defaults to <code>false</code>.  </li></ul><p><strong>Returns</strong></p><ul><li>If <code>latent</code> is <code>true</code>, returns a tuple containing:<ul><li><code>encoder</code>: The outputs of the encoder.</li><li><code>z</code>: The latent sample.</li><li><code>decoder</code>: The outputs of the decoder.</li></ul></li><li>If <code>latent</code> is <code>false</code>, returns the outputs of the decoder.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Define a VAE
vae = VAE(
    encoder=Flux.Chain(Flux.Dense(784, 400, relu), Flux.Dense(400, 20)),
    decoder=Flux.Chain(Flux.Dense(20, 400, relu), Flux.Dense(400, 784))
)

# Define input data
x = rand(Float32, 784)

# Perform the forward pass
outputs = vae(x, latent=true)</code></pre></div></section></article><h2 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.VAEs.loss" href="#AutoEncoderToolkit.VAEs.loss"><code>AutoEncoderToolkit.VAEs.loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">loss(
    vae::VAE,
    x::AbstractArray;
    β::Number=1.0f0,
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    kl_divergence::Function=encoder_kl,
    reg_function::Union{Function,Nothing}=nothing,
    reg_kwargs::NamedTuple=NamedTuple(),
    reg_strength::Number=1.0f0
)</code></pre><p>Computes the loss for the variational autoencoder (VAE).</p><p>The loss function combines the reconstruction loss with the Kullback-Leibler (KL) divergence, and possibly a regularization term, defined as:</p><p>loss = -⟨logπ(x|z)⟩ + β × Dₖₗ[qᵩ(z|x) || π(z)] + reg<em>strength × reg</em>term</p><p>Where:</p><ul><li>π(x|z) is a probabilistic decoder: π(x|z) = N(f(z), σ² I̲̲)) - f(z) is the function defining the mean of the decoder π(x|z) - qᵩ(z|x) is the approximated encoder: qᵩ(z|x) = N(g(x), h(x))</li><li>g(x) and h(x) define the mean and covariance of the encoder respectively.</li></ul><p><strong>Arguments</strong></p><ul><li><code>vae::VAE</code>: A VAE model with encoder and decoder networks.</li><li><code>x::AbstractArray</code>: Input data. The last dimension is taken as having each of the samples in a batch.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>β::Number=1.0f0</code>: Weighting factor for the KL-divergence term, used for annealing.</li><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function that computes the reconstruction log likelihood.</li><li><code>kl_divergence::Function=encoder_kl</code>: A function that computes the Kullback-Leibler divergence between the encoder output and a standard normal.</li><li><code>reg_function::Union{Function, Nothing}=nothing</code>: A function that computes the regularization term based on the VAE outputs. Should return a Float32. This function must take as input the VAE outputs and the keyword arguments provided in <code>reg_kwargs</code>.</li><li><code>reg_kwargs::NamedTuple=NamedTuple()</code>: Keyword arguments to pass to the   regularization function.</li><li><code>reg_strength::Number=1.0f0</code>: The strength of the regularization term.</li></ul><p><strong>Returns</strong></p><ul><li><code>T</code>: The computed average loss value for the input <code>x</code> and its reconstructed counterparts, including possible regularization terms.</li></ul><p><strong>Note</strong></p><ul><li>Ensure that the input data <code>x</code> matches the expected input dimensionality for the encoder in the VAE.</li></ul></div></section><section><div><pre><code class="language-julia hljs">loss(
    vae::VAE,
    x_in::AbstractArray,
    x_out::AbstractArray;
    β::Number=1.0f0,
    reconstruction_loglikelihood::Function=decoder_loglikelihood,
    kl_divergence::Function=encoder_kl,
    reg_function::Union{Function,Nothing}=nothing,
    reg_kwargs::NamedTuple=NamedTuple(),
    reg_strength::Number=1.0f0
)</code></pre><p>Computes the loss for the variational autoencoder (VAE).</p><p>The loss function combines the reconstruction loss with the Kullback-Leibler (KL) divergence and possibly a regularization term, defined as:</p><p>loss = -⟨logπ(x<em>out|z)⟩ + β × Dₖₗ[qᵩ(z|x</em>in) || π(z)] + reg<em>strength × reg</em>term</p><p>Where:</p><ul><li>π(x<em>out|z) is a probabilistic decoder: π(x</em>out|z) = N(f(z), σ² I̲̲)) - f(z) is</li></ul><p>the function defining the mean of the decoder π(x<em>out|z) - qᵩ(z|x</em>in) is the approximated encoder: qᵩ(z|x<em>in) = N(g(x</em>in), h(x_in))</p><ul><li>g(x<em>in) and h(x</em>in) define the mean and covariance of the encoder respectively.</li></ul><p><strong>Arguments</strong></p><ul><li><code>vae::VAE</code>: A VAE model with encoder and decoder networks.</li><li><code>x_in::AbstractArray</code>: Input data to the VAE encoder. The last dimension is taken as having each of the samples in a batch.</li><li><code>x_out::AbstractArray</code>: Target data to compute the reconstruction error. The last dimension is taken as having each of the samples in a batch.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>β::Number=1.0f0</code>: Weighting factor for the KL-divergence term, used for annealing.</li><li><code>reconstruction_loglikelihood::Function=decoder_loglikelihood</code>: A function that computes the reconstruction log likelihood.</li><li><code>kl_divergence::Function=encoder_kl</code>: A function that computes the Kullback-Leibler divergence.</li><li><code>reg_function::Union{Function, Nothing}=nothing</code>: A function that computes the regularization term based on the VAE outputs. Should return a Float32. This function must take as input the VAE outputs and the keyword arguments provided in <code>reg_kwargs</code>.</li><li><code>reg_kwargs::NamedTuple=NamedTuple()</code>: Keyword arguments to pass to the regularization function.</li><li><code>reg_strength::Number=1.0f0</code>: The strength of the regularization term.</li></ul><p><strong>Returns</strong></p><ul><li><code>T</code>: The computed average loss value for the input <code>x_in</code> and its reconstructed counterparts <code>x_out</code>, including possible regularization terms.</li></ul><p><strong>Note</strong></p><ul><li>Ensure that the input data <code>x_in</code> and <code>x_out</code> match the expected input dimensionality for the encoder in the VAE.</li></ul></div></section></article><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The <code>loss</code> function includes the <code>β</code> optional argument that can turn a vanilla VAE into a β-VAE by changing the default value of <code>β</code> from <code>1.0</code> to any other value.</p></div></div><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AutoEncoderToolkit.VAEs.train!" href="#AutoEncoderToolkit.VAEs.train!"><code>AutoEncoderToolkit.VAEs.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train!(vae, x, opt; loss_function, loss_kwargs, verbose, loss_return)</code></pre><p>Customized training function to update parameters of a variational autoencoder given a specified loss function.</p><p><strong>Arguments</strong></p><ul><li><code>vae::VAE</code>: A struct containing the elements of a variational autoencoder.</li><li><code>x::AbstractArray</code>: Data on which to evaluate the loss function. The last dimension is taken as having each of the samples in a batch.</li><li><code>opt::NamedTuple</code>: State of the optimizer for updating parameters. Typically initialized using <code>Flux.Train.setup</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>loss_function::Function=loss</code>: The loss function used for training. It should accept the VAE model, data <code>x</code>, and keyword arguments in that order.</li><li><code>loss_kwargs::NamedTuple=NamedTuple()</code>: Arguments for the loss function. These might include parameters like <code>σ</code>, or <code>β</code>, depending on the specific loss function in use.</li><li><code>verbose::Bool=false</code>: If true, the loss value will be printed during training.</li><li><code>loss_return::Bool=false</code>: If true, the loss value will be returned after training.</li></ul><p><strong>Description</strong></p><p>Trains the VAE by:</p><ol><li>Computing the gradient of the loss w.r.t the VAE parameters.</li><li>Updating the VAE parameters using the optimizer.</li></ol></div></section><section><div><pre><code class="language-julia hljs">    `train!(
        vae, x_in, x_out, opt; 
        loss_function, loss_kwargs, verbose, loss_return
    )`</code></pre><p>Customized training function to update parameters of a variational autoencoder given a loss function.</p><p><strong>Arguments</strong></p><ul><li><code>vae::VAE</code>: A struct containing the elements of a variational autoencoder.</li><li><code>x_in::AbstractArray</code>: Input data for the loss function. Represents an individual sample. The last dimension is taken as having each of the samples in a batch.</li><li><code>x_out::AbstractArray</code>: Target output data for the loss function. Represents the corresponding output for the <code>x_in</code> sample. The last dimension is taken as having each of the samples in a batch.</li><li><code>opt::NamedTuple</code>: State of the optimizer for updating parameters. Typically   initialized using <code>Flux.Optimisers.update!</code>.</li></ul><p><strong>Optional Keyword Arguments</strong></p><ul><li><code>loss_function::Function=loss</code>: The loss function used for training. It should accept the VAE model, data <code>x_in</code>, <code>x_out</code>, and keyword arguments in that order.  </li><li><code>loss_kwargs::NamedTuple=NamedTuple()</code>: Arguments for the loss function. These might include parameters like <code>σ</code>, or <code>β</code>, depending on the specific loss function in use.</li><li><code>verbose::Bool=false</code>: Whether to print the loss value after each training step.</li><li><code>loss_return::Bool=false</code>: Whether to return the loss value after each training step.</li></ul><p><strong>Description</strong></p><p>Trains the VAE by:</p><ol><li>Computing the gradient of the loss w.r.t the VAE parameters.</li><li>Updating the VAE parameters using the optimizer.</li></ol><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = Flux.setup(Optax.adam(1e-3), vae)
for (x_in, x_out) in dataloader
        train!(vae, x_in, x_out, opt) 
end</code></pre></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ae/">« Deterministic Autoencoders</a><a class="docs-footer-nextpage" href="../mmdvae/">MMD-VAE (InfoVAE) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Tuesday 23 July 2024 18:54">Tuesday 23 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
